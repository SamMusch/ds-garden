{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DataScience HOME","text":""},{"location":"#mocs","title":"MOCs","text":"<p>_ai-moc.md _ml-moc.md _ts-moc.md</p>"},{"location":"#review-resources","title":"Review Resources","text":"<p>Stanford: 3 separate courses. - AI: Reflex | States | Variables | Logic - ML: Supervised | Unsupervised | Deep | Tips - DL: CNN | RNN | Tips - Refreshers: algebra-calculus |probabilities-statistics</p>"},{"location":"#algorithm-types","title":"Algorithm Types","text":"<p>Regression - OLS - Logistic Regression - Multivariate Adaptive Regression Splines (MARS) - Locally Estimated Scatterplot Smoothing (LOESS)</p> <p>Instance-based Methods - k-Nearest Neighbour (KNN) - Learning Vector Quantization (LVQ) - Self-Organizing Map (SOM)</p> <p>Regularization Methods - Ridge Regression - LASSO - Elastic Net</p> <p>Decision Tree - Classification and Regression Tree (CART) - Iterative Dichotomiser 3 (ID3) - C4.5 - Random Forest - Gradient Boosting Machines (GBM)</p> <p>Bayesian - Naive Bayes - Averaged One-Dependence Estimators (AODE) - Bayesian Belief Network (BBN)</p> <p>Kernel Methods - Support Vector Machines (SVM) - Radial Basis Function (RBF) - Linear Discriminant Analysis (LDA)</p> <p>Association Rule Learning - Apriori algorithm - Eclat algorithm</p> <p>Artificial Neural Networks - Perceptron - Back-Propagation - Hopfield Network</p> <p>Deep Learning - Restricted Boltzmann Machine (RBM) - Deep Belief Networks (DBN) - Convolutional Network - Stacked Auto-encoders</p> <p>Dimensionality Reduction - Principal Component Analysis (PCA) - Partial Least Squares Regression (PLS) - Sammon Mapping - Multidimensional Scaling (MDS) - Projection Pursuit</p> <p>Ensemble Methods - Boosting - Bootstrapped Aggregation (Bagging) - AdaBoost - Stacked Generalization (blending) - Gradient Boosting Machines (GBM) - Random Forest</p> <p>dataview</p> <pre><code>TABLE without id\nfile.link\nfrom outgoing([](&lt;#&gt;))\n</code></pre>"},{"location":"AB-Testing/","title":"AB Testing","text":""},{"location":"AB-Testing/#terms","title":"Terms","text":"<p>Assume A is control, B is test.</p> <pre><code>Dependent Variable\n- Continuous\n- Proportions\n</code></pre> <pre><code>Are A &amp; B 2 separate, independent groups?\n- Yes --&gt; **Unpaired**\n- No --&gt; **Paired** (same group, multiple periods)\n</code></pre> <pre><code>Are we simply comparing B to a known benchmark?\n- Yes --&gt; **One-Sample**\n- No --&gt; **Two-Sample**\n</code></pre> <pre><code>Does B need to out-perform control?\n- Yes --&gt; **One-Sided**\n- No --&gt; **Two-Sided**\n</code></pre>"},{"location":"AI-MOC/","title":"AI MOC","text":"<pre><code>2025-05-20\nShows how I consolidated existing notes into these sections\nhttps://chatgpt.com/share/682cb713-d830-8000-8157-64a1af0e1cb7\n</code></pre> <p>ChatGPT: Paradigms of GenAI</p>"},{"location":"AI-MOC/#toc","title":"TOC","text":"<p>ChatGPT - Courses to take</p> Part Chapter Section Covers 0-0-neural-network-taxonomy.md 10 classes of NNs I \u2013 Core Concepts 1 Deep\u2011Learning Basics 1-1-why-nns-work.md What they are, layers, optimization 1-2-transformers-simple.md From seq to attention 1-3-self\u2011supervision-\u201clearning-without-labels\u201d.md Mask and predict 1-4-frameworks-pytorch-jax.md - 1-5-nlp-fundamentals.md HuggingFace 1-6-hugging-face-workflows.md HuggingFace II \u2013 Generative AI Essentials 2 Large Language Models 2-1-gpt\u20114o-llama-3.md 2.3-LoRA-Fine\u2011Tuning 2-4-gen-models-overview.md 2-5-autoencoders-gans-diffusionmodels.md 2-6-3blue1brown-explainer.md 3 Prompt Engineering 3.1 Crafting Prompts &amp; System Messages 3.2 Chain\u2011of\u2011Thought &amp; \u201cShow Your Work\u201d Tricks 3.3 Function Calling &amp; JSON Output (No Coding Headaches) 4 Retrieval\u2011Augmented Generation 4.1 RAG in One Picture 4.2 Vector Stores 101 (Pinecone, Qdrant) 4.3 Building Your First RAG Chatbot III \u2013 Practical MLOps 5 Deploying LLMs 5-1-local-vs-cloud-inference.md 5-2-quick-quantization-laptop-gpu.md 6 Data Pipelines for Gen\u2011AI 6-1-collect-clean-text-data.md 6-2-ci-cd-for-models.md IV \u2013 Responsible &amp; Edge AI 7 On\u2011Device AI 7.1 Running LLMs on Apple Silicon or a Phone 7.2 Privacy\u2011First Design Tips V \u2013 Applications &amp; Career 9 AI Agents &amp; Tools 9.1 ReAct, CrewAI &amp; Other Agent Patterns 9.2 Safeguarding Agents (Rate Limits, Guardrails) 11 Portfolio Projects 11.1 Build a Personal RAG Bot over Your Notes"},{"location":"AI-MOC/#explained","title":"Explained","text":"<pre><code>[ChatGPT](https://chatgpt.com/share/6817848d-fc2c-8000-acc3-e7610850caeb)\n\n(1) **Core\u202fConcepts**\nRefresh NN intuition. Need for transformers, self\u2011supervision, and modern frameworks like PyTorch.\n\n(2) **Generative\u202fAI\u202fEssentials**\nUnderstand and **use** LLMs, prompt engineering, and RAG.  \n\n(3) **Practical\u202fMLOps**\nDeploy, scale, and version models with minimal DevOps pain.  \n\n(4) **Responsible\u202f&amp;\u202fEdge\u202fAI**\nPrivacy/security for devices and constrained environments.\n\n(5) **Applications\u202f&amp;\u202fCareer**\nTie to real business use\u2011cases / continuous learning habits.\n</code></pre>"},{"location":"AI-MOC/#resources-cheats","title":"Resources / Cheats","text":"<p>Stanford Cheatsheets, Math</p>"},{"location":"ML-MOC/","title":"ML MOC","text":"<pre><code>TABLE without id\n\nfile.link,\n\nQuality,\n\n\nsum(HoursRemain) as Remain\nfrom outgoing([](&lt;#&gt;))\n</code></pre>"},{"location":"ML-MOC/#hide","title":"Hide","text":"Note Contains Status ml.md Cleaned deep-learning.md Cleaned supervised-2-classification.md supervised-4-regression-eval.md supervised-6-ensemble.md"},{"location":"TS-MOC/","title":"TS MOC","text":"<p>Deeper - theory-univariate.md - Univariate - video-series.md - Multivariate</p> <p>DL Resources - Book: Deep Learning for Time Series Forecasting by Jason Brownlee - Ch 6: Deep learning for text and sequences</p>"},{"location":"TS-MOC/#ts-ebook","title":"TS | Ebook","text":""},{"location":"TS-MOC/#02-taxonomy","title":"02. Taxonomy","text":"<pre><code>1. **Inputs vs. Outputs** (X vs Y)\n   - **Inputs**: Historical data provided to the model in order to make a single forecast.\n   - **Outputs**: Forecast for a future time step beyond the data provided as input.\n&lt;br&gt;\n1. **Endogenous vs. Exogenous** (Influencing each other?)\n   - **Endogenous**: Input variables that *are* influenced by other variables in the system and on which the output variable depends.  \n   - **Exogenous**: Input variables that *are not* influenced by other variables in the system and on which the output variable depends.\n&lt;br&gt;\n1. **Unstructured vs. Structured** (Time-dep patterns?)\n   - **Unstructured**: No obvious systematic time-dependent pattern in a time series variable.  \n   - **Structured**: Systematic time-dependent patterns in a time series variable (e.g. trend and/or seasonality).\n&lt;br&gt;\n1. **Univariate vs. Multivariate**\n   - Uni and Multi **Inputs**: 1+ input variables measured over time.\n   - Uni and Multi **Outputs**: 1+ output variables to be predicted.\n&lt;br&gt;\n1. **Single-step vs. Multi-step**\n   - **One-step**: Forecast the next time step.\n   - **Multi-step**: Forecast more than one future time steps.\n&lt;br&gt;\n1. **Static vs. Dynamic** (Streaming?)\n   - **Static**: Model is fit once and used to make predictions.\n   - **Dynamic**: Model is fit on newly available data prior to each prediction.\n&lt;br&gt;\n1. **Contiguous vs. Discontiguous** (Time uniform?)\n   - **Contiguous**: Observations are uniform over time.  (eg 1 per hour)\n   - **Discontiguous**: Observations are not uniform over time.\n</code></pre>"},{"location":"TS-MOC/#04-windows","title":"04. Windows","text":"<p>Sliding window: Take all columns in the dataset (including target variable) and take the lag.</p> <p>Parameters for the lag:</p> <ul> <li>Input Width: Number of time steps</li> <li>Offset: \"1\" if just using the values from previous time step</li> <li>Total width: Input Width + Offset</li> <li>Label width: How many timesteps in the future</li> </ul>"},{"location":"TS-MOC/#06-data-transform","title":"06. Data Transform","text":"<p>Input shape: - Samples: One sequence is one sample. A batch is comprised of one or more samples. - Time Steps: One time step is one point of observation in the sample. One sample is comprised of multiple time steps. - Features: One feature is one observation at a time step. One time step is comprised of one or more features.</p> <p>Put Simply: - Normal Shape: Rows, Columns - TS Shape: Rows, TimeSteps, Columns</p>"},{"location":"TS-MOC/#ch-20-lstms","title":"Ch 20:  LSTMs","text":"<p>Unlike other algorithms, LSTM RNNs are - capable of automatically learning features from sequence data, - support multiple-variate data, and - can output a variable length sequences that can be used for multi-step forecasting.</p> <p>References - Load dataset - ch 17 - Framework for evaluating models - ch 17     - Details of walk-forward validation - ch 19</p> <p>In this tutorial, we will explore a suite of LSTM architectures for multi-step time series forecasting. Specifically, we will look at how to develop the following models: - Vanilla LSTM model with vector output for multi-step forecasting with univariate input data. - Encoder-Decoder LSTM model for multi-step forecasting with univariate input data. - Encoder-Decoder LSTM model for multi-step forecasting with multivariate input data. - CNN-LSTM Encoder-Decoder model for multi-step forecasting with univariate input data. - ConvLSTM Encoder-Decoder model for multi-step forecasting with univariate input data.</p>"},{"location":"TS-MOC/#prep-vanilla","title":"Prep / vanilla","text":"<pre><code>LSTM shape: [**samples**, **timesteps**, **features**]. \n\n\nOne sample will be comprised of seven time steps with one feature for the seven days of total daily power consumed.\n[1, 7, 1]\n\n\nThe training dataset has 159 weeks of data, so the shape of the univariate training dataset would be: \n[159, 7, 1].\n</code></pre> <pre><code>**Create more training data** \n\n- Test problem: Predict daily consumption for the **next standard week** given the **prior standard week**\n- For training data only: Change the problem to predict the next 7 days given the prior 7 days, regardless of the standard week. \n\n**Flatten**\n- The training data is provided in standard weeks with 8 variables: [159, 7, 8]. \n- Need to flatten the data so we have 8 sequences.\n</code></pre> <pre><code># flatten data \ndata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n</code></pre> <pre><code>**Windowing**\n- For each feature, divide data into overlapping windows.\n- This means that instead of segmenting data into distinct weeks, each training instance slides by one day. (day 1 predicts day 8, day 2 predicts day 9, etc)\n\n\nNeed to keep track of start &amp; end indexes for the inputs &amp; outputs as we iterate across the length of the flattened data in terms of time steps.\n</code></pre> <pre><code># convert history into inputs and outputs \n\n# \"When we run this function on the entire training dataset, we transform 159 samples into 1,100\"\n# Since the last 6 days in this dataset don\u2019t have a complete output window, we can only use: \n# 1113\u22127+1 = 1100\n\ndef to_supervised(train, n_input, n_out=7):\n</code></pre> <pre><code>Small data, so small model\n\n- single **hidden** LSTM layer with 200 units.\n- fully **connected** layer with 200 nodes that will interpret the features learned by the LSTM layer. \n- **output** layer will directly predict a vector with seven elements, one for each day in the output sequence.\n\nSpecs\n- Loss : MSE\n- Optimizer = Adam\n- Epochs: 70\n- Batch size: 16\n</code></pre> <pre><code># The function below \n  # prepares the training data, \n  # defines the model, and \n  # fits the model on the training data, returning the fit model ready for making predictions.\n\ndef build_model(train, n_input):\n</code></pre> <pre><code>**walk-forward validation**\n\n**What is it?**\n- Ccommon evaluation method\n- Instead of training once and making all predictions at once, the model is **retrained over time**, updating with new observations and making one forecast at a time.\n\n**How does it work here?** \n- The model uses the **past week\u2019s observations** (7 days) to predict the **next week** (7 days).\n- After making a prediction, the model gets the **actual observed values** from that week and adds them to the dataset before predicting the following week.\n</code></pre>"},{"location":"TS-MOC/#encoder-decoder-lstm-with-univariate-input","title":"Encoder-Decoder LSTM With Univariate Input","text":"<pre><code>| Feature    | **Vanilla LSTM**                                              | **Encoder-Decoder LSTM**                                             |\n| ---------- | ------------------------------------------------------------- | -------------------------------------------------------------------- |\n| Output     | A full sequence is predicted **in one step**                  | The sequence is predicted **one step at a time**                     |\n| Processing | LSTM reads the entire input and outputs a **vector** directly | LSTM first encodes the input, then **iteratively** generates outputs |\n| State      | No feedback from previous outputs                             | **Decoder uses prior predictions** to influence the next step        |\n</code></pre> <pre><code>**Key Idea of Encoder-Decoder**\n- The **encoder** reads the input sequence and compresses it into a **fixed-length vector representation**.\n- The **decoder** takes this representation and generates **one time step at a time**, using its internal state to remember prior predictions.\n\n**Why Does This Matter?**\n\n- **Vanilla LSTM** treats **each time step in the output as independent**, meaning it doesn\u2019t explicitly use previous outputs when generating future ones.\n- **Encoder-Decoder LSTM** allows the model to **remember what was predicted in previous time steps** and adjust the next predictions accordingly. This is useful in **multi-step forecasting**, where the prediction for one day can influence the prediction for the next.\n</code></pre>"},{"location":"TS-MOC/#encoder-decoder-lstm-with-multivariate-input","title":"Encoder-Decoder LSTM With Multivariate Input","text":""},{"location":"_assets/Deep-Learning/","title":"Deep Learning","text":"","tags":["ml_"]},{"location":"_assets/Deep-Learning/#general-neural-networks","title":"GENERAL NEURAL NETWORKS","text":"<p>https://www.tensorflow.org/tutorials</p> <p>From class notes</p> <p>Links - Ch 13 Code: CNNs - Ch 14 Code: RNNs - Andrew Ng Code: Multi-Class &amp; Neural Nets - Andrew Ng Code: Neural Nets</p> <p>The activation function is a hyperparameter, the weights and biases are parameters.</p>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#ch-10","title":"Ch 10","text":"<p>In the first part of this chapter, we will introduce artificial neural networks, starting with a quick tour of the very first ANN architectures, leading up to Multi-Layer Per\u2010 ceptrons (MLPs) which are heavily used today (other architectures will be explored in the next chapters). In the second part, we will look at how to implement neural networks using the popular Keras API.</p>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#anns","title":"ANNs","text":"<p>Warren McCulloch and Walter Pitts proposed a very simple model of the biological neuron, which later became known as an artificial neuron: it has one or more binary (on/off) inputs and one binary output. The artificial neuron simply activates its out\u2010 put when more than a certain number of its inputs are active. McCulloch and Pitts showed that even with such a simplified model it is possible to build a network of artificial neurons that computes any logical proposition you want.</p> <p>Pg 283: Artificial neuron contains: - 1+ binary input neurons (IN) - Input connections between IN &amp; ON - 1 binary output neuron (ON) - If a threshold number of connections are reached, the ON is activated</p>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#perceptron","title":"Perceptron","text":"<p>The Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt.</p> <p>Table from Pg 293: Typical Regression MLP Architecture</p> hyperparameter Typical value ## input neurons One per input feature ## hidden layers Typically 1-5 ## neurons per hidden layer Typically 10-100 ## output neurons 1 per prediction dim (1 for regress, 2 for binary, etc) Hidden activation ReLu or SELU Output activation None for regression. ReLu/softplus Loss function MSE or MAE/Huber (if outliers) &gt; An MLP is composed of one passthrough input layer, one or more layers of TLUs (threshold logic units), called hidden layers, and one final layer of TLUs called the output layer. The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. Every layer except the output layer includes a bias neuron and is fully connected to the next layer. The architecture that the signal flows only in one direction from the inputs to the outputs is called feedforward neural network (FNN). When an ANN (artificial neural network) contains a deep stack of hidden layers, it is called a deep neural network (DNN). - pg 287 --- <p>Pg 284: Perceptron (figure on pg 286)</p> <p>Perceptrons are based on a slightly different type of artificial neuron called a threshold logical unit (TLU). Instead of using binary \"off/on\", the inputs &amp; outputs are numbers.</p> <p>Steps: 1. TLU computes weighted sum of inputs (IN &amp; input weight).    $z = w_1 x_1 + w_n x_n = X^T w$ 2. TLU applies a step function (sigmoid, tangent, or ReLu) to this sum.    $h_w(x) = step(z)$,     where     $z=X^T w$</p> <p>1 single TLU layer is called a Perceptron.</p>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#equation","title":"Equation","text":"<p>Outputs of fully connected layer = $h_{W,b} (X) = \\Theta(XW + b)$</p> <p>X = our dataset (matrix of input features) - 1 row per instance - 1 column per feature</p> <p>W = weight matrix - 1 row per input neuron (IN) - 1 column per artifical neuron (AN) in the layer</p> <p>b = bias vector, contains all connection weights between bias neuron &amp; AN - 1 bias term per AN</p> <p>$\\Theta$ = activation function</p>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#mlp-backpropagation","title":"MLP Backpropagation","text":"<p>An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the output layer.</p> <p>Note: The Perceptron learning algorithm is the same thing as Stochastic Gradient Descent assuming the following parameters: - <code>loss</code> = 'perceptron' - <code>learning_rate</code> = 'constant' - <code>eta0</code> = 1 (the learning rate) - <code>penalty</code> = None (no regularization)</p> <p>Pg 290: Backpropagation is Gradient Descent but using an efficient technique for computing the gradients automatically. - Forward | Make prediction, measure total error - Backward | (in reverse) Go through each layer to measure each connection's error contribution - Gradient descent | Tweak connection weights</p> <p>Backpropagation computes the gradients of cost function for every model parameter using reverse-mode autodiff 1. (Forward) Feed into network 2. For each layer, the output is found based on connection (weight &amp; bias)    Note that the connection is not linear so that we can take derivative using the chain rule. 3. Finds total network error 4. (Backwards) Uses chain rule to find how much each connection contributed to total error working from final layer to initial layer 5. (Gradient descent) Adjust the connection weights  </p>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#code-explanation","title":"Code explanation","text":"<p>Pg 299-316</p>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#hyperparameters","title":"Hyperparameters","text":"<p>Pg 323 | Paper by Leslie Smith</p> <ul> <li># hidden layers: Start with 1 or 2 hidden layers. Early layers find simple patterns, later layers find complex. Add until we start overfitting.</li> <li># neurons per hidden layer: Typically use the same for each (100), but could try adding more neurons to early layers if needed.</li> <li>Learning rate: Start by training the model with 300 iterations and a low learning rate ($10^{-5}$) and gradually increase it to 10.</li> <li>Optimizer: Ch 11</li> <li>Batch size: 32</li> <li>Activation function: ReLU for hidden layers, output layer depends on task</li> <li># of iterations: Don't worry about it, use Early Stopping instead</li> </ul>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#tips-for-training-nn","title":"Tips for training NN","text":"<p>Scaling input data - Standardscaler for numeric - Categorial/Ordinal Guide</p> <p>If bad on training: - Use different activation function (try maxout) - Use different learning rate optimizer</p> <p>If good on train but bad on test (overfitting): - Regularization - apply penalty in the loss function if weight and bias is too high from layer to layer    - L1 subtracts which is why we are able to get rid of irrelevant features    - L2 discounts, which is why the features don't reach 0 - Early stop - makes regularization not that important - limits epoch - need to be run on validation set    - When we increase epoch, we will repeat GD many times. This will decrease error for training data, but we are looking for the min testing error. - Drop out    - Use separate mini batches - remove a certain percent from each training batch for each layer (Need to multiply all weights by <code>1 - drop %</code>)    - Training we drop out some neurons, in testing we bring them back and discount their weights</p>","tags":["ml_"]},{"location":"_assets/Deep-Learning/#why-not-just-add-more-layers","title":"Why not just add more layers?","text":"<ul> <li>Vanishing gradient - First layers not nearly as impacted as later stages because much large variance in later stages</li> <li>ReLU - A change in the slope from one neuron to the next will have the same impact on each resulting layer</li> <li>Problem of Relu - Change of zero will lead to the resulting neuron dying out though</li> <li>Alternative - Leaky ReLU - very small instead of 0</li> <li>Alternative - Parametric ReLU - can adjust slope for the \"below 0\" section</li> <li>Batch normalization - Makes sure that the changing distribution of prev layer's inputs aren't impacting us</li> <li>Gradient clipping (for grad explosion) - set a threshold that gradients can't go above/below</li> </ul>","tags":["ml_"]},{"location":"_assets/ML/","title":"ML","text":"<p>[!summary] Summary Consolidates ML foundations.</p> <p>Textbook online PDF</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#deeper-links","title":"Deeper Links","text":"Notes Contains supervised-2-classification.md Models, evaluation, and some code supervised-4-regression-eval.md Evaluation supervised-6-ensemble.md Models deep-learning.md","tags":["ml_","supervised"]},{"location":"_assets/ML/#overview-cards","title":"Overview #cards","text":"<p>[!quote] AI is rooted in mathematics and statistics. When creating an ANN, we're conducting mathematical operations on data represented in linear space; it is,\u00a0by nature, applied mathematics and statistics.\u00a0ML  algorithms are nothing but function approximations; they try and find a mapping between an input and a correct corresponding output. We use algebraic methods to create algorithms that learn these mappings.</p> <p>Almost all ML can be expressed in a fairly straight-forward formula; bringing together a dataset and model, along with a loss function and optimization technique that are applicable to the dataset and model. Source</p> Term Cake Analogy Explanation ML Examples ML Definition Data Ingredients. Raw dataset Method Deciding to bake a specific type of cake (e.g., birthday cake). Supervised learning, classification The broader framework. Combines algorithms, techniques, and strategies to solve a problem. Algorithm The recipe you follow to bake the cake. Defines the step-by-step process. For XGBoost: Gradient BoostingFor NNs: Backpropagation The procedure or set of rules used to train the model. It defines how the model learns from data. Model The finished cake, ready to serve. Represents what has been learned. For XGBoost: Collection of DTsFor NNs: Network of neurons with learnable weights and biases The output or representation of what has been learned. Loss Function The taste test. Measures how close the cake is to the desired flavor. MSE, cross-entropy Optimization Technique Adjusting the recipe to improve the cake\u2019s flavor (minimize error). For XGBoost: second-order gradient boostingFor NNs: Adam, etc How you iteratively modify parameters (weights, biases) to minimize error","tags":["ml_","supervised"]},{"location":"_assets/ML/#introduction","title":"Introduction","text":"","tags":["ml_","supervised"]},{"location":"_assets/ML/#data-mining-tasks","title":"Data Mining Tasks","text":"<p>1) Classification: which set of class does this person belong to? 2) Regression: How many hours will this person use our service? 3) Similarity matching: These firms bought from us. Who else is likely to? 4) Clustering: Which segments do our customers fall into? 5) Co-occurence (market basket analysis): For each segment, what are commonly purchased together? 6) Profiling: What is the typical behavior of this segment? 7) Link prediction: You and x share 10 friends. She likes this person, so you prob will too. 8) Data reduction: Dropping unnecessary info thats clouding our insights 9) Causal modeling: What influences our DV?</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#data-mining-process","title":"Data Mining Process","text":"<p>The ML lifecycle includes several steps for transforming data into actionable insights. 1. Business Understanding: Define the problem &amp; success criteria. 2. Data Understanding: How was it collected? Any implicit biases? 3. Data Preparation 4. Model     1. Data Preparation (possibly)     2. Modeling (possibly) 5. Evaluate 6. Deploy</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#types-of-systems","title":"Types of Systems","text":"<pre><code>There are so many different types of ML systems that it is useful to classify them in broad categories based on:\n\n- Are they trained with human supervision? \n   - **Supervised**: Where data includes labels for learning.\n   - **Unsupervised**: Where data lacks explicit labels.\n   - **Semisupervised**: \n\n   - **Reinforcement**: Where an agent learns by interacting with an environment.\n\n- Can they learn incrementally on the fly?\n   - **Online**: Yes\n   - **Batch**: No\n\n- How do they generalize?\n   - **Instance-based**: Can simply comparing new data points to known data points\n   - **Model-based learning**: Require training data to detect patterns\n</code></pre>","tags":["ml_","supervised"]},{"location":"_assets/ML/#human-supervision","title":"Human Supervision","text":"","tags":["ml_","supervised"]},{"location":"_assets/ML/#supervised-learning","title":"Supervised Learning","text":"<p>Dataset contains both inputs &amp; corresponding labels. Tasks include: - Classification: Discrete categories. - Regression: Continuous values.</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Identifies patterns without labeled data. Key methods are: - clustering.md: Group similar data points (k-means, Hierarchical Cluster Analysis (HCA), Expectation Maximization). - dimensionality-reduction.md: Simplify datasets without losing too much info.     - PCA | Principal Component Analysis     - Kernel PCA     - LLE | Locally-Linear Embedding     - t-SNE | t-distributed Stochastic Neighbor Embedding - anomaly-detection.md: Identify deviations from normal behavior. - association-rules.md (Apriori, Eclat)</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#semi-supervised","title":"Semi-Supervised","text":"<p>Some data is labeled, some isn't. (Typically lots of unlabeled data + some labeled data) - Most of these algorithms are combinations of unsupervised &amp; supervised algorithms. - Deep belief networks (DBNs), restricted Boltzmann machines (RBMs)</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Process: The learning system (aka agent) 1. Observes environment 2. Performs actions 3. Gets rewarded With trial-and-error, it teaches itself the best strategy (ie policy) to max reward.</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#learning-type-batch-online","title":"Learning Type | Batch &amp; Online","text":"<p>Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Pg 41</p> Learning Type When Training Process Adaptability <code>BATCH</code> Static Train on the entire dataset at once Slow <code>ONLINE</code> Streaming Model updates iteratively as new data points or mini-batches of data are received. (<code>Learning rate</code> is key.) Fast ### Generalization Type Model &amp; Instance &gt; One more way to categorize Machine Learning systems is by how they generalize. Pg 44 <ul> <li>Model-Based Learning: Use training data to build a model, then extrapolate.</li> <li>Instance-Based Learning: Use known problems as initial points. Predict new problems based on similarity to old ones.<ul> <li>Examples: Case-Based Reasoning, Radial Basis Function Networks, Locally Weighted Regression, Memory-Based Collaborative Filtering, Prototype-Based Learning     All these methods depend heavily on stored data or a local region of the feature space. Predictions or decisions are derived directly or indirectly from comparisons to similar instances.</li> </ul> </li> </ul> FEATURE Explanation Prediction Adaptability When <code>Instance</code> Learn \"by heart\", think look-alikes Using a similarity metric. Predictions are <code>locally-informed</code>. See example. High - changes predictions quickly Intuitive for business usersLocal relationships more important than global <code>Model</code> Learns over time with new training data Using learned parameters Low - adding drops to an ocean Generalization to unseen data is key","tags":["ml_","supervised"]},{"location":"_assets/ML/#instanced-based-details","title":"Instanced-Based | Details","text":"<p>Analogy: Think spam detection. Ideally, every spam email would be the same. Model learns this \"by heart\", then flags all identical emails. (In reality, use a measure of similarity between 2 emails.)</p> <p>Steps for instance-based regression using k-NN: 1. Find the k-NN:</p> <pre><code>- For a given query instance, calculate distance from all the training instances.\n- Select $k$. (The number of instances to use.) These are the \"neighbors\" of the query point.\n</code></pre> <ol> <li>Predict: Using these neighbors, compute the average of their target value. Use this as your prediction. (Could use more advanced technique than just average.)</li> </ol>","tags":["ml_","supervised"]},{"location":"_assets/ML/#challenges-of-ml","title":"Challenges of ML","text":"<p>In short, since your main task is to select a learning algorithm and train it on some data, the two things that can go wrong are \u201cbad algorithm\u201d and \u201cbad data.\u201d  Pg 50</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#data-issues","title":"Data Issues","text":"<ol> <li>Quantity: Typically need thousands of examples</li> <li>Quality: Might have too much info missing, could be poorly collected</li> <li>Non-representative: When old cases no longer reflect new cases. Sources:     Sampling noise: Data is too small     Sampling bias: Sampling method is flawed.</li> <li>Irrelevant features. Solutions:     Feature selection: Select only most useful features.     Feature extraction: Combine existing features to produce meaningful ones.     New features: Use external sources to create new features.</li> </ol>","tags":["ml_","supervised"]},{"location":"_assets/ML/#algorithm-issues","title":"Algorithm Issues","text":"<ol> <li> <p>Overfitting solutions:</p> <ul> <li>Select a model with fewer parameters</li> <li>Feature reduction</li> <li>Constrain the model (regularization)</li> <li>Gather more data</li> <li>Reduce noise in training data</li> <li>Underfitting: Model is too simplistic to capture underlying patterns.</li> <li>Select a more powerful model, with more parameters</li> <li>Feeding better features to the learning algorithm (feature engineering)</li> <li>Reducing the constraints on the model (e.g., reducing the regularization hyper\u2010parameter)</li> </ul> </li> </ol>","tags":["ml_","supervised"]},{"location":"_assets/ML/#tuning-evaluation","title":"Tuning &amp; Evaluation","text":"","tags":["ml_","supervised"]},{"location":"_assets/ML/#process-testing-validating","title":"Process | Testing &amp; Validating","text":"<p>1) Split: Split data into train &amp; test. (Usually 80% for training.) 2) Validation set: Use nested k-fold CV to split up training set. 3) Train set: Run multiple models x hyperparameters 4) Train set: Select models x hyperparameter combo with best performance on validation set. 5) Test set: Find generalization error for an estimate of performance on unseen data.     1) Training good + validation bad = overfitting (Image)     2) Validation good + test bad = overfitting     3) Validation bad + test bad = learning rate too high.</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>[!quote] Hyperparameters are configuration variables that tell the model what methods to use, as opposed to\u00a0model parameters\u00a0which are learned during training.</p> <p>Fine-tuning hyperparameters is critical for optimal model performance: - Grid Search: Exhaustive search over parameter combinations. - Random Search: Randomly sample parameters to find optimal settings. - Bayesian Optimization: Use probabilistic models to select parameters.</p> <p>Model-type details:</p> Type # of parameters Complexity Scalability with Data Size Hyperparameters PARAMETRIC Fixed Lower, less risk of overfitting. Limited 1. Regularization terms (L1/L2)2. Learning rate3. Small # of key params NONPARAMETRIC Unconstrained, grows with size. High, prone to overfitting. High Focus on selecting right complexity (e.g., depth of trees).","tags":["ml_","supervised"]},{"location":"_assets/ML/#code","title":"Code","text":"<pre><code># Grid search\nsearch = GridSearchCV(estimator = rf_classifier, param_grid = parameters, cv = 3)\n\n# Apply to training\nsearch.fit(x_train, y_train)  \nsearch.best_params_\n\n# Best combo\nbest = search.best_estimator_  \naccuracy = evaluate(best, x_test, y_test)\n</code></pre>","tags":["ml_","supervised"]},{"location":"_assets/ML/#cross-validation","title":"Cross-Validation","text":"<p>Cross-validation splits data to validate models. Methods include: - k-Fold: Divides data into $k$ subsets for training and testing. - Nested: Addresses hyperparameter overfitting by adding an outer validation loop.</p>","tags":["ml_","supervised"]},{"location":"_assets/ML/#nested-k-fold","title":"Nested K-Fold","text":"<p>Removes overfit \"leak\" from evaluating on train set. - Use when hyperparameters also need to be optimized - Estimates generalization error of the underlying model &amp; hyperparameters Process - Inner loop: Fits model to each training set, then select hypers over validation set - Outer loop: Estimates generalization error by averaging test set scores over several dataset splits</p>","tags":["ml_","supervised"]},{"location":"_assets/AI-Book-open-from-MOC/0-0-Neural-Network-Taxonomy/","title":"0.0 Neural Network Taxonomy","text":""},{"location":"_assets/AI-Book-open-from-MOC/0-0-Neural-Network-Taxonomy/#classes-of-nns","title":"Classes of NNs","text":"<p>ChatGPT: Task \u2192 Classes - Images \u2192 CNNs or Vision Transformers - Language \u2192 RNN-based or Transformer-based models - Graphs \u2192 GNNs - Generating new data \u2192 GANs, VAEs - Dimensionality reduction \u2192 Autoencoders, RBMs</p> Class Use Cases Key Idea Examples 1. Feedforward NNs Basic classification and regression Information flows in a single direction (input \u2192 output); no feedback loops MLP 2. Convolutional NNs Image recognition, computer vision, text classification Uses convolution and pooling layers to capture spatial/temporal features LeNet, AlexNet, ResNet 3. Recurrent NNs Language modeling, speech recognition, time-series forecasting Maintains a hidden state that evolves over time, processing data sequences LSTM, GRU (Gated Recurrent Unit) 4. Transformers - 1-2-transformers-simple.md Language modeling, machine translation, text summarization, multimodal tasks Relies on self-attention to process sequential data in parallel \u201cAttention Is All You Need,\u201d BERT, GPT series 5. Autoencoders Dimensionality reduction, anomaly detection, generative modeling Learns compressed representations (encodings) and reconstructs data from them Denoising Autoencoders, Sparse Autoencoders, Variational AEs 6. Generative Adversarial Networks Image generation, data augmentation, style transfer Employs two competing networks: a Generator and a Discriminator, trained in a zero-sum game DCGAN, CycleGAN, StyleGAN 7. Graph NNs Social networks, molecular graph analysis, recommendation systems Operates on graph-structured data, capturing node and edge relationships Graph Convolutional Network (GCN), GraphSAGE, GAT 8. Boltzmann Machines Feature learning, collaborative filtering (e.g., Netflix Prize solutions) Stochastic, energy-based models learning internal data representations Restricted Boltzmann Machines (RBMs) 9. Spiking NNs Low-power computation, robotics, real-time processing in IoT Uses spikes/timing-based signals, mimicking biological neural behavior Neuromorphic hardware implementations 10. Other Specialized Architectures Hierarchical image representation, continuous time modeling, structured NLP tasks Various advanced or niche models targeting specific improvements or use cases Capsule Networks, Neural ODEs, Recursive NNs"},{"location":"_assets/AI-Book-open-from-MOC/1-1-Why-NNs-Work/","title":"1.1 Why NNs Work (Intuition)","text":"<p>Resources - deep-learning.md: Class notes, more detailed than below. - ChatGPT: for this markdown</p> <pre><code>**Summary**\nNNs work because they are **flexible function approximators** that learn useful **internal representations** of data by stacking layers of transformations. \nInstead of manually designing features or rules, neural nets **learn patterns directly from data** through a process of trial-and-error optimization.\n\n| Strength                        | What It Means                                     |\n| ------------------------------- | ------------------------------------------------- |\n| Hierarchical representation     | Learns from simple to complex features            |\n| Flexible function approximation | Can learn a wide variety of patterns and tasks    |\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-1-Why-NNs-Work/#why-nns-work","title":"Why NNs Work","text":"<pre><code>- Neural nets can **approximate almost any function**, given enough layers and data (Universal Approximation Theorem).\n- They are **data-driven**: no need to hand-engineer features or rules.\n- They are **modular** (think sub-task) and **scalable**: you can adjust size, depth, and architecture based on the problem.\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-1-Why-NNs-Work/#what-are-nns","title":"What Are NNs?","text":"<pre><code>At their core, NNs are inspired by the brain. They consist of layers of artificial \u201cneurons\u201d that **process input data** and pass signals forward. Each neuron learns to activate in response to specific patterns.\n\n- **Input layer**: Receives raw data (e.g., pixels, words, numbers).\n- **Hidden layers**: Transform and abstract the input features into higher-level representations.\n- **Output layer**: Produces a prediction or decision.\n\nEach connection has a **weight** that determines how much influence one neuron has on the next.\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-1-Why-NNs-Work/#layered-representations","title":"Layered Representations","text":"<pre><code>The true magic of NNs lies in **stacking multiple layers**:\n\n- **First layers** learn low-level features (e.g., edges in images).\n- **Deeper layers** build on those to detect more abstract patterns (e.g., faces, objects).\n- **Final layers** make task-specific predictions (e.g., cat vs. dog).\n\nThis **hierarchical learning** allows neural nets to go from raw data to meaning, automatically.\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-1-Why-NNs-Work/#learning-through-feedback-training","title":"Learning Through Feedback (Training)","text":"<pre><code>NNs improve by **learning from mistakes**. Here's how:\n\n1. Make a guess (prediction).\n2. Compare it to the correct answer (loss).\n3. Adjust internal weights to reduce future error.\n\nThis process is called **gradient-based optimization**.\n- (Note: Backpropagation is part of gradient-based optimization. Specifically, it\u2019s the algorithm used to compute how to adjust the weights by calculating the gradients (step 3). Gradient-based optimization is the overall process; backpropagation is the mechanism used during that process.)\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-2-Transformers-Simple/","title":"1.2 Transformers in Plain English","text":"<p>Resources - 1-5-nlp-fundamentals.md: Talks about transformers - ChatGPT: This markdown</p>"},{"location":"_assets/AI-Book-open-from-MOC/1-2-Transformers-Simple/#everyday-analogy","title":"Everyday Analogy","text":"<pre><code>Imagine reading a paragraph and instantly knowing which earlier sentences are important. \nThat\u2019s what transformers do \u2014 they:\n1. read left-to-right AND\n2. weigh relevance across the whole passage in real time\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-2-Transformers-Simple/#what-is-a-transformer","title":"What is a Transformer?","text":"<pre><code>Transformers are \n- a type of DL model\n- the **architecture** behind nearly every language model today.\n\nThey look at entire sentences (or documents) all at once.\n</code></pre> <pre><code>Transformers marked a shift \nfrom **sequence**-based models \nto **attention**-based models.\n- **Attention** allows models to see everything at once \u2014 a breakthrough for understanding **context**.\n</code></pre> <p>Intro to Attention is all you Need</p> <p>The dominant sequence transduction models are based on complex RNNs or CNNs that include an encoder &amp; a decoder. The best performing models also connect the encoder &amp; decoder through an attention mechanism.</p> <p>We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</p>"},{"location":"_assets/AI-Book-open-from-MOC/1-2-Transformers-Simple/#why-they-matter","title":"Why They Matter","text":"<pre><code>Traditional models like RNNs had trouble (1) remembering long-term dependencies or (2) were slow to train. \nTransformers solved both problems:\n- **Parallel Processing**: They process words simultaneously, not sequentially.\n- **Long-Range Context**: They can \"attend\" to any word, no matter how far apart.\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-2-Transformers-Simple/#key-ingredients-without-equations","title":"Key Ingredients (Without Equations)","text":"<pre><code>**Self-attention is like scanning a room and deciding who\u2019s worth listening to** \u2014 for every word in the sentence.\n\n\n- **Tokens**: Break input into chunks (usually words or subwords).\n- **Embeddings**: Convert those chunks into numbers.\n- **Self-Attention**: Each token \"looks\" at others to decide what matters.\n- **Layers**: These operations repeat multiple times to refine understanding.\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-2-Transformers-Simple/#remaining-questions-research","title":"Remaining Questions #research","text":"<ol> <li>Is it only transformers that have tokens/embeddings/self-attention/layers?</li> <li>Meaning of \"language\" models - 1:1 correspondence with LLMs?</li> <li>Note the \"encoder/decoder\" mentioned for sequence-based models. Could be key break out.</li> </ol>"},{"location":"_assets/AI-Book-open-from-MOC/1-3-Self%E2%80%91Supervision-%E2%80%9CLearning-Without-Labels%E2%80%9D/","title":"1.3 Self\u2011Supervision   \u201cLearning Without Labels\u201d","text":""},{"location":"_assets/AI-Book-open-from-MOC/1-3-Self%E2%80%91Supervision-%E2%80%9CLearning-Without-Labels%E2%80%9D/#context","title":"Context","text":"<pre><code>- **Supervised**: learns a direct mapping from inputs to human-provided labels.\n- **Unsupervised**: discovers structure or representation in unlabeled data.\n- **Semi-supervised**: uses a small set of labels plus a large pool of unlabeled examples to improve performance.\n- **Self-supervised (this doc)**: automatically creates its own labels from the data (e.g. masking, rotation prediction) to learn representations without manual annotation.\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-3-Self%E2%80%91Supervision-%E2%80%9CLearning-Without-Labels%E2%80%9D/#business-analogy","title":"Business Analogy","text":"<pre><code>**Puzzle-Building Analogy**  \n\n1. Someone put together a 1k piece puzzle.\n2. They hide a patch of 10 pieces.\n3. You need to predict what those 10 pieces look like.\n4. Repeat 2 &amp; 3\n5. Over time, you get better at **understanding** the overall image, all without **seeing** the full completed picture.\n\n\n- Hiding puzzle pieces = Masking inputs\n- Predicting them yourself = Creating your own training labels\n- Getting better at the full image = Learning rich representations\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-3-Self%E2%80%91Supervision-%E2%80%9CLearning-Without-Labels%E2%80%9D/#high-level-concept","title":"High-Level Concept","text":"<p>Self\u2011supervision is a way for models to generate their own \u201cquiz\u201d signals from raw data, eliminating the need for manually annotated examples. It powers breakthroughs in how systems understand patterns and adapt to new tasks.</p>"},{"location":"_assets/AI-Book-open-from-MOC/1-3-Self%E2%80%91Supervision-%E2%80%9CLearning-Without-Labels%E2%80%9D/#technical-details-for-data-scientists","title":"Technical Details for Data Scientists","text":"<ul> <li>Proxy Tasks: Create artificial tasks using the data itself (e.g., mask part of an input and predict it).</li> <li>Representation Learning: Learn embeddings by contrasting different views of the same data point or reconstructing missing pieces.</li> <li> <p>Common Approaches:  </p> </li> <li> <p>Masked token prediction in text (e.g., hide 15% of words).  </p> </li> <li>Masked patch reconstruction in images (e.g., hide random image patches).  </li> <li>Contrastive objectives that pull related samples together in embedding space and push others apart.</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/1-3-Self%E2%80%91Supervision-%E2%80%9CLearning-Without-Labels%E2%80%9D/#connections-to-other-topics","title":"Connections to Other Topics","text":"<ul> <li>Transfer Learning: Use self\u2011supervised pretraining to initialize models before fine\u2011tuning on a specific task.</li> <li>Contrastive Learning: A popular self\u2011supervised method that underlies many representation\u2011learning models.</li> <li>RAG and Downstream Tasks: Better embeddings from self\u2011supervision improve retrieval and generation in RAG pipelines.</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/1-3-Self%E2%80%91Supervision-%E2%80%9CLearning-Without-Labels%E2%80%9D/#key-techniques","title":"Key Techniques","text":"<ul> <li>Contrastive Learning </li> <li>InfoNCE loss, SimCLR, MoCo  </li> <li>Encourages similar views to have closer embeddings than dissimilar ones</li> <li>Masked Language Modeling (MLM) </li> <li>BERT-style masking of input tokens  </li> <li>Predicts masked tokens from context</li> <li>Masked Image Modeling (MIM) </li> <li>MAE-style masking of image patches  </li> <li>Reconstructs missing patches</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/1-3-Self%E2%80%91Supervision-%E2%80%9CLearning-Without-Labels%E2%80%9D/#popular-methods","title":"Popular Methods","text":"<ul> <li>SimCLR (Chen et al., 2020)  </li> <li>MoCo (He et al., 2020)  </li> <li>BERT (Devlin et al., 2019)  </li> <li>MAE (He et al., 2021)</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/","title":"1.4 Modern Frameworks (PyTorch, JAX)","text":""},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#high-level","title":"High-Level","text":""},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#business-analogy","title":"Business Analogy","text":"<pre><code>- **PyTorch**: kitchen that lets chefs experiment with recipes, then creates ready-to-serve meals. \n- **JAX**: a factory line that automates recipe execution at scale\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#high-level-concept","title":"High-Level Concept","text":"<p>Modern ML frameworks combine ease of use with performance optimizations. - PyTorch 2.x emphasizes flexible, Python\u2011style development with optional compilation for speed. - JAX offers a functional approach, automatically parallelizing and optimizing computations.</p>"},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#connections-to-other-topics","title":"Connections to Other Topics","text":"<ul> <li>Quantization and Deployment: Framework compilation features simplify converting models to lighter formats for edge devices.  </li> <li>MLOps Pipelines: Integrates seamlessly with CI/CD workflows by scripting model definitions as pure functions.  </li> <li>Hardware Awareness: Both frameworks adapt to different accelerators\u2014GPUs, TPUs, and even mobile chips\u2014enabling on\u2011device AI.</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#details","title":"Details","text":""},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#pytorch","title":"PyTorch","text":"<ul> <li>torch.compile   Compiles eager models into optimized kernels via TorchInductor and AOTAutograd.</li> <li>Dynamic Eager Execution   Allows Python-native debugging and control flow, with optional compilation.</li> <li>TorchScript Hybridization   Export parts of the model to an intermediate representation for deployment.</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#jax","title":"JAX","text":"<ul> <li>Functional NumPy API   Drop-in replacement for NumPy with <code>jit</code>, <code>grad</code>, <code>vmap</code>, <code>pmap</code>.</li> <li>XLA Compilation   Just-In-Time-compiles code for CPU, GPU, and TPU targets.</li> <li>Purely Functional Paradigm   Emphasizes immutability and function transformations.</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#feature-comparison","title":"Feature Comparison","text":"Feature PyTorch 2.x JAX Compilation <code>torch.compile</code> <code>jax.jit</code> &amp; XLA Autograd Eager Autograd Engine Functional <code>grad</code> API Vectorization Manual/<code>torch.vmap</code> Automatic <code>vmap</code> Parallelism <code>DistributedDataParallel</code> <code>pmap</code>"},{"location":"_assets/AI-Book-open-from-MOC/1-4-Frameworks-PyTorch-JAX/#quick-start-examples","title":"Quick Start Examples","text":"<pre><code># PyTorch 2.x compilation\nimport torch\nmodel = torch.nn.Linear(10, 5)\ncompiled_model = torch.compile(model)\n</code></pre> <pre><code># JAX jit and grad\nimport jax\nimport jax.numpy as jnp\n\ndef loss_fn(x):\n    return jnp.sum(x ** 2)\n\njit_loss = jax.jit(loss_fn)\ngrad_loss = jax.grad(loss_fn)\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/1-5-NLP-Fundamentals/","title":"1.5 NLP Fundamentals","text":"<p>Back/Outgoing Links <pre><code>list from [](&lt;#&gt;) and !outgoing([](&lt;#&gt;))\n</code></pre></p> <pre><code>Most common NLP methods\n\n1. **Symbolic (Rule-Based):** Not really used anymore because of the advance of LLMs.\n\n2. **ML (Statistical &amp; Neural)**\n</code></pre>","tags":["ai_"]},{"location":"_assets/AI-Book-open-from-MOC/1-5-NLP-Fundamentals/#huggingface-course","title":"HuggingFace Course","text":"<p>For context: 1-2-transformers-simple.md</p> <p>HuggingFace  |  Wikipedia - NLP | field of linguistics &amp; ML related to language. The aim is to understand single words &amp; the context of those words together. - LLMs are DL algorithms that use transformer models to perform NLP tasks.</p>","tags":["ai_"]},{"location":"_assets/AI-Book-open-from-MOC/1-5-NLP-Fundamentals/#1-transformer-models","title":"1. Transformer Models","text":"<p>3 major groups of transformer models: History, 2018-2021</p> Model Examples Tasks Encoder BERT Auto-Encoding Receives input, builds features. ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa Sentence classification, named entity recognition, extractive question answering Decoder GPT Auto-Regressive Uses encoder's features + other inputs to generate target. CTRL, GPT, GPT-2, Transformer XL, LLaMA Text generation Encoder-decoder BART Sequence-to-sequence BART, T5, Marian, mBART Summarization, translation, generative question answering <pre><code> All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as ***language models***. This means they have been trained on large amounts of raw text in a **self-supervised** fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!\n\nThe general pretrained model then goes through ***transfer learning*** where the model is fine-tuned in a supervised way \u2014 that is, using human-annotated labels \u2014 on a given task.\n</code></pre>","tags":["ai_"]},{"location":"_assets/AI-Book-open-from-MOC/1-5-NLP-Fundamentals/#transfer-learning","title":"Transfer Learning","text":"<ul> <li>Pretraining is the act of training a model from scratch.</li> <li>Fine-tuning is done after a model has been pretrained, using a new specific dataset.</li> </ul> <p>Transformer models are built with attention layers, which tell the model to pay specific attention to certain words in the sentence. (Paper)</p> <p>Transformer Model Terms: - Architecture: Skeleton of the model \u2014 the definition of each layer &amp; operation within the model. (eg <code>BERT</code>) - Checkpoints: The weights that will be loaded in the architecture. (eg <code>bert-lower-cased</code>) - Model: Umbrella term that could mean \u201carchitecture\u201d or \u201ccheckpoint\u201d. This course will specify architecture or checkpoint when it matters.</p>","tags":["ai_"]},{"location":"_assets/AI-Book-open-from-MOC/1-5-NLP-Fundamentals/#2-using-transformers","title":"2. Using Transformers","text":"<p>Moved to 1-6-hugging-face-workflows.md</p>","tags":["ai_"]},{"location":"_assets/AI-Book-open-from-MOC/1-6-Hugging-Face-Workflows/","title":"1.6 Hugging Face Workflows","text":"<p>Behind the pipeline - steps - YouTube</p> <ol> <li> <p>Preprocessing with a tokenizer:</p> </li> <li> <p>Load the tokenizer.</p> </li> <li>Tokenize input text into tokens and convert to tensors.</li> <li> <p>Model:</p> </li> <li> <p>Load the pretrained model.</p> </li> <li>Pass the tokenized inputs through the model to get output logits.</li> <li> <p>Postprocessing the output:</p> </li> <li> <p>Convert logits to probabilities using softmax.</p> </li> <li>Map the probabilities to labels.</li> </ol> <p>Breaking out steps in detail: Tokenizer 1. Tokens | Split the input into tokens (ie words, subwords, punctuation) 2. Special tokens | Add special tokens such as \"sentence begin\". 3. Input IDs | Map each token to it's unique ID from that specific pre-trained model. (This is the checkpoint which comes out as a dictionary.)</p> <p>Breaking out steps in detail: Model (Image) 1. Input from Tokenizer 2. Transformer network    3. Embeddings | convert each unique ID into vector    4. Layers | manipulate vectors using attention mechanism 5. Hidden states (features) | 6. Head | take high-dimensional vector of hidden states as input, then convert predictions to task-specific output 7. Output for post-processing</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-1-GPT%E2%80%914o-Llama-3/","title":"2.1 What Makes GPT\u20114o & Llama 3 Tick (High Level)","text":"<p>Resources - ChatGPT: For this document.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-1-GPT%E2%80%914o-Llama-3/#overview","title":"Overview","text":"<pre><code>GPT\u20114o (OpenAI) and LLaMA 3 (Meta) represent two of the most advanced LLMs currently available. \nBoth models follow the **transformer** architecture but differ in design philosophy, training data, and usage priorities.\n\n- **GPT\u20114o** is optimized for commercial-scale AI applications with cutting-edge multimodal reasoning\n- **LLaMA 3** is designed for flexible research &amp; local deployment. \n</code></pre> <pre><code>Distinct approaches in openness, specialization, and end-use design.\n\n| FEATURE | Developer | Multimodal | Open Source          | Max Model Size | Inference           | Fine-tuning         | Use Cases                |\n| ------- | --------- | ---------- | -------------------- | -------------- | ------------------- | ------------------- | ------------------------ |\n| GPT\u20114o  | OpenAI    | Yes        | No                   | Not disclosed  | Cloud-first         | Proprietary methods | ChatGPT, Copilot         |\n| LLAMA 3 | Meta      | No         | Yes (non-commercial) | 70B            | Local &amp; Cloud-ready | LoRA / QLoRA (open) | Research, OSS assistants |\n\n\n- **Multimodal** means the model can understand and generate _multiple types of input and output_, such as **text/image/audio**\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/2-1-GPT%E2%80%914o-Llama-3/#same","title":"Same","text":"<pre><code>- **Transformer**: Both use the transformer decoder stack with self-attention and feedforward layers.\n- **Pretraining Objective**: Trained using next-token prediction on massive datasets.\n- **Tokenization**: Use byte pair encoding (BPE) or variants.\n- **Inference Mode**: Both support autoregressive text generation, often used in chat-based applications.\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/2-1-GPT%E2%80%914o-Llama-3/#differences","title":"Differences","text":""},{"location":"_assets/AI-Book-open-from-MOC/2-1-GPT%E2%80%914o-Llama-3/#gpt4o","title":"GPT\u20114o","text":"<pre><code>- **Multimodal**: GPT\u20114o is natively multimodal\u2014trained to process text, audio, image, and video in a unified model.\n- **Training Emphasis**: Heavy focus on aligning model outputs with human preferences via reinforcement learning from human feedback (RLHF).\n- **Usage Context**: Best in class for general-purpose, multilingual, and reasoning-heavy tasks.\n- **Deployment**: Used in OpenAI\u2019s commercial products (ChatGPT, Copilot).\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/2-1-GPT%E2%80%914o-Llama-3/#llama-3","title":"LLaMA 3","text":"<pre><code>- **Open-Weight**: LLaMA 3 is released under a non-commercial license, enabling researchers to experiment and deploy with fewer constraints.\n- **Variants**: Released in sizes like 8B and 70B, with a focus on efficiency and open availability.\n- **Training Data**: Trained on 15T+ tokens, including public web data (filtered), books, code, and academic sources.\n- **Specialization**: Emphasizes interpretability and community fine-tuning via LoRA and QLoRA.\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/2-4-Gen-Models-Overview/","title":"2.4 Generative Models Overview","text":"<p>Textbook | Oreilly</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-4-Gen-Models-Overview/#overview","title":"Overview","text":"<p>!pasted-image-20241231152903.png.md Image comes from textbook in 2-5-autoencoders-gans-diffusionmodels.md</p> <p>Generative models are enabling computers to have an understanding of the world. They are: - Unsupervised - Data generators</p> <p>We'll focus on the two most popular types of models: - The\u00a0variational autoencoder\u00a0(VAE) - The generative adversarial network\u00a0(GAN) - (Also touch upon other common generative models)</p> <p>VAEs &amp; GANs. - Each relies on condensing data --&gt; generating from this condensed data. - Both are probabilistic models, meaning that they rely on inference from probability distributions in order to generate data</p> <p>When writing advanced models in TensorFlow, remember: - Layers: what kind you need - Activation function: what type you need - Shape: shape your data through the network - Procedures: for example, think choosing loss functions &amp; optimizers</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-4-Gen-Models-Overview/#autoencoder-overview-separate-resource","title":"Autoencoder Overview - Separate Resource","text":"<p>ChatGPT - <code>Quick summary. Meant to be intuitive, not exhaustive.</code></p> <p>Autoencoders learn succinct representations. They serve as a foundation for many generative and dimensionality-reduction techniques. Structurally, autoencoders consist of an\u00a0input layer, a\u00a0hidden layer, and an\u00a0output layer: pasted-image-20241231143947.png.md</p> <pre><code>**Intuition**\n\n**Autoencoders**: example of encoder --&gt; decoder.\n\n1. I have an **idea** that I want to share. I'll explain using an example. Three examples come to mind, and I choose the simplest one.\n2. The listener hears my example, and then re-generates my original **idea**.\n\nWe want the **idea** I shared to be as close as possible to the **idea** the listener understands. (\"Reconstruction loss\")\n</code></pre> <p>Autoencoders are a self-supervised approach to representation learning. They operate with two main components: 1. An encoder that compresses (or encodes) the input into a smaller, more compact representation (an \u201cinformation bottleneck\u201d). 2. A decoder that reconstructs the original input from this compact representation. The encoder is a recognition network, the decoder is a generative network.</p> <p>Key Ideas - Information Bottleneck: The encoder forces the network to keep only the most relevant aspects of the data; this helps avoid simply memorizing inputs. - Reconstruction Loss: Typically mean squared error or cross entropy, which measures how close the decoder\u2019s output is to the original input. - Goal: Find a balance where the model accurately reconstructs input data but doesn\u2019t overfit or memorize it.</p> <p>Connection to PCA - Like PCA, autoencoders perform dimensionality reduction, taking high-dimensional data and learning a lower-dimensional representation. - Difference: Autoencoders can capture nonlinear relationships, whereas PCA is restricted to linear correlations.</p> <p>Manifolds - A manifold is a continuous, non-intersecting surface (think of a sphere). - In neural networks, loss functions and data structures often lie on (possibly complex) manifolds. Autoencoders learn to navigate these manifolds by mapping input data into a meaningful, compressed space and then reconstructing it back.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/","title":"2.5 Autoencoders, GANs & Diffusion Models (Hands\u2011On)","text":"<pre><code>May20: Renamed from \"Ch 17 Autoencoders, GANs, and Diffusion Models\"\n\nNote to self - created March 22, 2025. \n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#autoencoders","title":"Autoencoders","text":"<p>Also see 2-4-gen-models-overview.md - different textbook Moved to 2-4-gen-models-overview#overview---separate-resource.md</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#ch-17","title":"Ch 17","text":"<p>Autoencoders are ANNs capable of learning latent representations without supervision. - Latent representations (aka codings): dense representations of the input data.</p> <p>Autoencoders uses - Dimensionality reduction: Since codings typically have a lower dimensionality than the input data - Feature detectors: Can be used for unsupervised pretraining of DNNs - Generation: Some are\u00a0generative models: they are capable of randomly generating new data that looks very similar to the training data</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#gans","title":"GANs","text":"<p>GANs (Generative Adversarial Networks) are composed of 2 NNs that compete 1. generator to generate data similar to training data 2. discriminator that tries to tell real data from fake data Intuition: criminal generates fake money, cop tries to id fake money Adversarial training: training competing NNs</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#autoencoders-vs-gans","title":"Autoencoders vs GANs","text":"<p>Both - Unsupervised - Learn dense representations - Can be used for generation - Similar applications</p> <p>Work differently: - Autoencoders: simply learn to copy their inputs --&gt; outputs (non-trivial) - GANs: create 2 NNs that compete with each other</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#variational-autoencoders","title":"Variational Autoencoders","text":"<p>These are different than the other autoencoders in the chapter: - They are probabilistic - ie, their outputs are partly determined by chance. - They are generative - ie, they can produce new data</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#getting-to-ai-generative-models","title":"Getting to AI - generative models","text":"<p>Classes of neural networks (goals): - Discriminative models: focus on learning boundaries or correlations between inputs &amp; labels.     - (e.g., typical feedforward nets, CNNs for classification, RNNs for sequence labeling) - Generative models: focus on modeling the underlying data distribution, allowing them to produce new (synthetic) data or fill in missing features given partial information.     - (e.g., autoencoders, VAEs, GANs)</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#autoencoders-for-inspiration","title":"Autoencoders (For Inspiration)","text":"<p>Summarized in 2-5-autoencoders-gans-diffusionmodels#autoencoders.md</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#building-an-autoencoder","title":"Building an Autoencoder","text":"<p>If you're thinking that the task of reconstructing an output doesn't appear that useful, you're not alone.\u00a0</p> <p>What exactly do we use these networks for? Autoencoders help to extract features when there are no known labeled features at hand.</p> <p>To illustrate how this works, let's walk through an example using TensorFlow. We're going to reconstruct the MNIST dataset here, and, later on, we will compare the performance of the standard autoencoder against the variational autoencoder in relation to the same task.</p> <p>Let's get started with our imports and data. MNIST is contained natively within TensorFlow, so we can easily import it: <pre><code>import tensorflow as tf  \nimport numpy as np  \n\nfrom tensorflow.examples.tutorials.mnist import input_data  \nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n</code></pre></p> <p>For ease, we can build the auto-encoder with the\u00a0<code>tf.layers</code>\u00a0library. We'll want our Autoencoder architecture to\u00a0follow the convolutional/de-convolutional pattern, where the input layer of the decoder matches the size of the input and the subsequent layer squash the data into a smaller and smaller representation. The decoder will be the same architecture reversed, starting with the small representation and working larger.</p> <p>All together, we want it to look something like the following: !pasted-image-20241231144552.png.md</p> <p>Let's start with the encoder; we'll define an initializer for the the weight and bias factors first, and then define the encoder as a function that takes and input, x. we'll then use the\u00a0<code>tf.layers.dense</code>\u00a0function to create standard, fully connected neural network layers. The encoder will have three layers, with the first layer size matching the input dimensions of the input data (<code>784</code>), with the subsequent layers getting continually smaller: <pre><code>initializer = tf.contrib.layers.xavier_initializer()  \ndef encoder(x): \n    input_layer = tf.layers.dense(\n        inputs=x, units=784, activation=tf.nn.relu,\n        kernel_initializer=initializer, bias_initializer=initializer)  \n    z_prime = tf.layers.dense(\n        inputs=input_layer, units=256, activation=tf.nn.relu,\n        kernel_initializer=initializer, bias_initializer=initializer)  \n    z = tf.layers.dense(\n        inputs=z_prime, units=128, activation=tf.nn.relu,\n        kernel_initializer=initializer, bias_initializer=initializer)  \n    return z\n</code></pre></p> <p>Next, let's let's build our decoder; it will be using the same layer type and initializer as the encoder, only now we invert the layers, so that the first layer of the decoder is the smallest and the last is the largest. <pre><code>def decoder(x):  \n    x_prime_one = tf.layers.dense(\n        inputs=x, units=128, activation=tf.nn.relu,\n        kernel_initializer=initializer, bias_initializer=initializer)  \n    x_prime_two = tf.layers.dense(\n        inputs=x_prime_one, units=256, activation=tf.nn.relu,\n        kernel_initializer=initializer, bias_initializer=initializer)  \n    output_layer = tf.layers.dense(\n        inputs=x_prime_two, units=784, activation=tf.nn.relu,\n        kernel_initializer=initializer, bias_initializer=initializer)  \n    return output_layer\n</code></pre></p> <p>Before we get to training, let's define some hyper-parameters that will be needed during the training cycle. We'll define the size of our input, the learning rate, number of training steps, the batch size for the training cycle, as well as how often we want to display information about our training progress. <pre><code>input_dim = 784   \nlearning_rate = 0.001  \nnum_steps = 1000  \nbatch_size = 256  \ndisplay = 1\n</code></pre></p> <p>We'll then define the placeholder for our input data so that we can compile the model: <pre><code>x = tf.placeholder(\"float\", [None, input_dim])\n</code></pre></p> <p>And subsequently, we compile the model and the optimizer as you've seen before in previous chapter: <pre><code># Construct the full autoencoder  \nz = encoder(x)  \n\n## x_prime represents our predicted distribution  \nx_prime = decoder(z)   \n\n# Define the loss function and the optimizer  \nloss = tf.reduce_mean(tf.pow(x - x_prime, 2))  \noptimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n</code></pre></p> <p>Lastly, we'll code up the training cycle. By this point, most of this should be fairly familiar to you; start a TensorFlow session, and iterate over the epochs/batches, computing the loss and accuracy at each point: <pre><code>with tf.Session() as sess:  \n    sess.run(tf.global_variables_initializer())  \n\n    ## Training Loop  \n    for i in range(1, num_steps+1):  \n\n        ## Feed Batches of MNIST Data  \n        batch_x, _ = mnist.train.next_batch(batch_size)  \n\n        ## Run the Optimization Process  \n        _, l = sess.run([optimizer, loss], feed_dict={x: batch_x})  \n\n        ## Display the loss at every 1000 out of 30,000 steps  \n        if i % display == 0 or i == 1:  \n            print('Step %i: Loss: %f' % (i, l))\n</code></pre></p> <p>For this particular example, we'll add in a little something more to this process; a way to plot the reconstructed images alongside their original versions. Keep in mind that this code is still contained within the training session, just outside of the training loop: <pre><code>n = 4  \ncanvas_orig = np.empty((28 * n, 28 * n))  \ncanvas_recon = np.empty((28 * n, 28 * n))  \n\nfor i in range(n):  \n\n    batch_x, _ = mnist.test.next_batch(n)  \n\n    # Encode and decode each individual written digit  \n    g = sess.run(decoder, feed_dict={x: batch_x})  \n\n    # Display original images  \n    for j in range(n):  \n\n        # Draw the original digits  \n        canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = batch_x[j].reshape([28, 28])  \n\n    # Display reconstructed images  \n    for j in range(n):  \n\n        # Draw the reconstructed digits  \n        canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])  \n\n# Plot the original image vs the reconstructed images.   \nprint(\"Original Images\")  \nplt.figure(figsize=(n, n))  \nplt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")  \nplt.show()  \n\nprint(\"Reconstructed Images\")  \nplt.figure(figsize=(n, n))  \nplt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")  \nplt.show()\n</code></pre></p> <p>After training, you should end up with a result along the lines of the following, with the actual digits on the left, and the reconstructed digits on the right: !pasted-image-20241231145350.png.md</p> <p>So what have we done here? By training the autoencoder on unlabeled digits, we've done the following:\u00a0 - Learned the latent features of the dataset without having explicit labels - Successfully learned the distribution of the data and reconstructed the image from scratch, from that distribution</p> <p>Now, let's say that we wanted to take this further and generate or classify new digits that we haven't seen yet. To do this, we could remove the decoder and attach a classifier or generator network: !pasted-image-20241231145424.png.md</p> <p>The encoder therefore becomes a means of initializing a supervised training model.\u00a0Standard autoencoders have been used in a variety of tasks. In the supplementary code for this chapter, we'll walk through an example where we utilize autoencoders for visual anomaly detection.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#variational-autoencoders-vae","title":"Variational autoencoders (VAE)","text":""},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#overview","title":"Overview","text":"<p>Variational autoencoders\u00a0(VAEs) are built on the idea of the standard autoencoder, and are powerful generative models and one of the most popular means of learning a complicated distribution in an unsupervised fashion. VAEs are\u00a0probabilistic models\u00a0rooted in Bayesian inference. A probabilistic model is exactly as it sounds:</p> <p>Probabilistic models incorporate random variables and probability distributions into the model of an event or phenomenon.</p> <p>VAEs, and other generative models, are probabilistic\u00a0in that they seek to learn a distribution that they utilize for subsequent sampling. While all generative models are probabilistic models, not all probabilistic models are generative models.</p> <p>The probabilistic\u00a0structure of VAEs comes into play with their encoders. Instead of building an encoder that outputs a single value to describe the input data, we want to learn the latent variables by generating a probability distribution for each of those variables. VAEs have a constraint on the encoding network that forces it to generate vectors that roughly follow a standard normal distribution.\u00a0This is what makes VAEs unique: they generate from continuous space, which means that we can easily sample and interpret from that space. We'll see how this unique probabilistic structure helps us to overcome the limitations of standard autoencoders.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#structure","title":"Structure","text":"<p>Like standard autoencoders, VAEs utilize the same encoder/decoder framework, but, that aside, they are mathematically different from their namesake. VAEs take a probabilistic perspective in terms of guiding the network: !pasted-image-20241231145555.png.md</p> <p>Both our\u00a0encoder\u00a0and\u00a0decoder\u00a0networks are generating distributions from their input data. The encoder generates a distribution from its training data,\u00a0Z, which then becomes the input distribution for the decoder. The decoder takes this distribution,\u00a0Z, and tries to replicate the original distribution,\u00a0X, from it.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#encoder","title":"Encoder","text":"<p>The encoder generates its distribution by first defining its prior\u00a0as a standard normal distribution. Then, during training, this distribution becomes updated, and the decoder can easily sample from this distribution later on. Both the encoder and the decoder are unique in terms of VAEs in that they output two vectors instead of one: a\u00a0vector of means,\u00a0\u03bc, and another vector of standard deviation,\u00a0\u03c3.\u00a0These help to define the limits for our generated distributions. Intuitively, the mean vector controls where the encoding of an input should be centered, while the standard deviation controls the extent to which the encoding may vary from the mean. This constraint on the encoder forces the network to learn a distribution, thereby taking it beyond the vanilla autoencoder that simply reconstructs its output.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#decoder","title":"Decoder","text":"<p>Like the standard autoencoder, the decoder in the VAE is a backward convolutional network, or a deconvolutional network. In processing the decoding, data is sampled from the generation stochastically (randomly), making the VAE one of the few models that can directly sample a probability distribution without a Markov chain Monte Carlo method.\u00a0As a result of the stochastic generation process, the encoding that we generate from each pass will be a different representation of the data, all while maintaining the same mean and standard deviation. This helps with the decoder's sampling technique; because all encodings are generated from the same distribution, the decoder learns that a latent data point and its surrounding points are all members of the same class. This allows the decoder to learn how to generate from similar, but slightly varying, encodings.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#training-and-optimizing-vaes","title":"Training and optimizing VAEs","text":"<p>VAEs utilize a negative\u00a0log-likelihood loss as their reconstruction loss to measure how much information is lost during the reconstruction phase of the decoder. If the decoder does not reconstruct the input satisfactorily, it will incur a large reconstruction loss.\u00a0VAEs also introduce something called\u00a0Kullback\u2013Leibler\u00a0(KL) divergence\u00a0into their loss functions. KL divergence simply measures how much two probability distributions diverge; in other words, how different they are from one another. We want to minimize the KL distance between the mean and standard deviation of the target distribution and that of a standard normal. It is properly minimized when the mean is zero and the standard deviation is one. The log-likelihood loss with KL divergence forms the complete loss function for VAEs.</p> <p>When training VAEs, there is an implicit trade-off between the accuracy of the model and how close it can model the normal distribution. On its own, KL loss results in encoded data that is densely clustered near the center of the distribution, with little iteration with other potentially similar encoded data. A decoder wouldn't be able to decode anything from the space, because it wouldn't be particularly continuous! By combining the losses and optimizing them, we are able to preserve the dense nature of encoded data created by the KL loss function, as well as the clustered data produced by the reconstruction loss. What we then end up with are tight clusters that are easy for the decoder to work with. We wanted our generated distribution Z to resemble a standard normal distribution as closely as possible, and the\u00a0more efficiently we can encode the original image, the closer we can push the standard deviation of the generated distribution toward one, the standard deviation of the targeted normal distribution.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#utilizing-a-vae","title":"Utilizing a VAE","text":"<p>We can construct a variational autoencoder in TensorFlow to see how it compares to it's simpler, standard autoencoder cousin. In this section, we'll be using the same MNIST dataset so that we can standardize our comparison across methods.\u00a0Let's walk through how to construct a VAE by utilizing it to generate handwriting based on the\u00a0MNIST\u00a0dataset. Think of\u00a0x\u00a0as being the individual written characters and\u00a0z\u00a0as the latent features in each of the individual characters that we are trying to learn.</p> <p>First, let's start with our imports: <pre><code>import numpy as np  \nimport tensorflow as tf  \nfrom tensorflow.examples.tutorials.mnist import input_data\n</code></pre></p> <p>As before, we can import the\u00a0data\u00a0directly from the TensorFlow library: <pre><code>mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n</code></pre></p> <p>Next, we can start to build the encoder. We're going to be utilizing the same\u00a0<code>tf.layers</code>\u00a0package as we did before. Here, our encoder will look fairly similar to how it did in the previous example, our layers will take in an input and gradually compress that input until we generate a latent distribution,\u00a0z: <pre><code>def encoder(x):\n    input_layer = tf.layers.dense(\n        inputs=x, units=784, activation=tf.nn.elu,\n        kernel_initializer=initializer, bias_initializer=initializer,name='input_layer')\n\n    hidden_1 = tf.layers.dense(\n        inputs=input_layer, units=256, activation=tf.nn.elu,\n        kernel_initializer=initializer, bias_initializer=initializer)\n\n    hidden_2 = tf.layers.dense(\n        inputs=hidden_1, units=128, activation=tf.nn.elu,\n        kernel_initializer=initializer, bias_initializer=initializer)\n</code></pre></p> <p>Here's where we start to diverge from the standard autoencoder, however. While the last layer in the encoder will give us the potential z-distribution that represents our data, we'll need to calculate the values of\u00a0$\\mu$ and\u00a0$\\sigma$\u00a0that will help define that distribution. We can do that by creating two new layers that take in the potential distribution z, and output out values of\u00a0<code>mu</code>\u00a0and\u00a0<code>sigma</code>: <pre><code>mu = tf.layers.dense(inputs=z, units=10, activation=None)  \nsigma = tf.layers.dense(inputs=z, units=10, activation=None)\n</code></pre></p> <p>Next, we'll use these values to go ahead and calculate the KL divergence for the encoder, which will eventually go into constructing our final loss function: <pre><code>kl_div = -0.5 * tf.reduce_sum( 1 + sigma - tf.square(mu) - tf.exp(sigma), axis=1)  \n\nkl_div = tf.reduce_mean(latent_loss)\n</code></pre></p> <p>Let's go ahead and create the decoder portion of the variational autoencoder now; we'll create a deconvolutional pattern that reverses the dimensions of the encoder. All of this will be contained under the function below: <pre><code>def decoder(z, initializer):\n    layer_1 = fully_connected(\n        z, 256, scope='dec_l1', activation_fn=tf.nn.elu, \n        kernel_initializer=initializer, bias_initializer=initializer)\n    layer_2 = fully_connected(\n        layer_1, 384, scope='dec_l2', activation_fn=tf.nn.elu,\n        kernel_initializer=initializer, bias_initializer=initializer)\n    layer_3 = fully_connected(\n        layer_2, 512, scope='dec_l3', activation_fn=tf.nn.elu,\n        kernel_initializer=initializer, bias_initializer=initializer)\n    dec_out = fully_connected(\n        layer_3, input_dim, scope='dec_l4', activation_fn=tf.sigmoid,\n        kernel_initializer=initializer, bias_initializer=initializer)\n</code></pre></p> <p>Also under the decoder function, we'll use the decoder output to calculate the reconstruction loss: <pre><code>epsilon = 1e-10  \n\nrec_loss = -tf.reduce_sum(x * tf.log(epsilon + dec_out) + (1 - x) * tf.log(epsilon + 1 - dec_out), axis=1)  \n\nrec_loss = tf.reduce_mean(rec_loss)\n</code></pre></p> <p>As usual, we'll prepare our training parameters before we start initializing the model. We'll define a learning rate, batch size for our training, the number of training epochs, dimension of the input, and the size of our total training sample: <pre><code>learning_rate = 1e-4  \nbatch_size = 100  \nepochs = 100  \ninput_dim = 784   \nnum_sample = 55000  \nn_z = 10\n</code></pre></p> <p>We'll also define the placeholder for our input data,\u00a0x: <pre><code>x = tf.placeholder(name='x', dtype='float', shape=[None, input_dim])\n</code></pre></p> <p>Before we start training, we'll initialize the model, loss, and\u00a0optimizer: <pre><code>## initialize the models  \nz, kl_div = encoder(x)  \ndec_out, rec_loss = decoder(x)  \n\n## Calculate the overall model loss term  \nloss = tf.reduce_mean(rec_loss + kl_div)  \n\n## Create the optimizer  \noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)  \n\n## Create the weight initializer  \ninitializer = tf.contrib.layers.xavier_initializer()\n</code></pre></p> <p>Finally, we can run the actual training process. This we be similar to the training processes that we've already built and experienced: <pre><code>with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        for iter in range(num_sample // batch_size):\n            batch_x = mnist.train.next_batch(batch_size)\n            _, l, rl, ll = sess.run([optimizer, loss, rec_loss, kl_div], feed_dict={x: batch_x[0]})\n\n        if epoch % 5 == 0:\n            print('[Epoch {}] Total Loss: {}, Reconstruction Loss: {}, Latent Loss: {}'.format(epoch, l, rl, ll))\n</code></pre></p> <p>Lastly, we can use the bit of code following code to generate new samples from our newly trained model: <pre><code>z = np.random.normal(size=[batch_size, n_z])  \nx_generated = x_hat = self.sess.run(dec_out, feed_dict={z: z})  \n\nn = np.sqrt(batch_size).astype(np.int32)  \nI_generated = np.empty((h*n, w*n))  \nfor i in range(n):  \n    for j in range(n):  \n        I_generated[i*h:(i+1)*h, j*w:(j+1)*w] = x_generated[i*n+j, :].reshape(28, 28)  \n\nplt.figure(figsize=(8, 8))  \nplt.imshow(I_generated, cmap='gray')\n</code></pre></p> <p>Ultimately, you should end up with an image such as the following, with the original digits on the left and the generated digits on the right. Observe how much clearer the digits are compared to the original autoencoder. Now, let's see how we can take this further with GANs.\u00a0</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#generative-adversarial-networks-gan","title":"Generative adversarial networks (GAN)","text":""},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#overview_1","title":"Overview","text":"<p>Generative adversarial\u00a0networks (GANs) are a class of networks that were introduced by Ian Goodfellow in 2014. In GANs, two neural networks play off against one another as adversaries in an\u00a0actor-critic model, where one is the creator and the other is the scrutinizer. The creator, referred to as the\u00a0generator network, tries to create samples that will fool the scrutinizer, the discriminator network. These two increasingly play off against one another, with the generator network creating increasingly believable samples and the discriminator network getting increasingly good at spotting the samples. In summary:</p> <ul> <li>The generator tries to maximize the probability of the discriminator passing its outputs as real, not generated</li> <li>The discriminator guides the generator to create ever more realistic samples</li> </ul> <p>All in all, this process is represented as follows: !pasted-image-20241231150923.png.md</p> <p>GANs can be used for a variety of tasks, and, in recent years, many GAN varieties have been created. As they were originally built for image-related tasks, we will focus our architecture discussions on image-based GANs. A larger list of GANs is available at the end of the section. Throughout, we'll follow along in TensorFlow to illuminate the topics. As before, we'll be utilizing the same MNIST data in order to compare the frameworks with our previous ones: <pre><code>import tensorflow as tf  \nimport numpy as np\n\nfrom tensorflow.examples.tutorials.mnist import input_data  \nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)  \ntraining_data = (mnist.train.images - 0.5) / 0.5\n</code></pre></p> <p>With that, let's walk through the pieces of the network one at a time. By this point, you should be pretty familiar with this process in TensorFlow.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#discriminator-network","title":"Discriminator network","text":"<p>The discriminator network in image-related GANs is a standard convolutional neural network. It takes in an image and outputs a single number that tells us whether the image is\u00a0real\u00a0or\u00a0fake. The discriminator takes in an image, and learns the attributes of that image so that it may be a good\u00a0judge\u00a0vis-\u00e0-vis the outputs of the generator.\u00a0In TensorFlow, we can create the\u00a0<code>discriminator</code>\u00a0as a function that we will then run in a TensorFlow session later on. This framework is more or less the same as you've seen in the previous sections with autoencoder and variational autoencoders; we'll use the higher level\u00a0<code>tf.layers</code>\u00a0api to create three main network layers and an output layer. After each of the main network layers, we'll add a dropout layer for regularization. The last layer will be slightly different, as we'll want to squash the output. For this, we'll use a sigmoid activation function that will give us a final output saying if an image is believed to be fake or not: <pre><code>def discriminator(x, initializer, dropout_rate):\n    layer_1 = tf.layers.dense(\n        x, units=1024, activation=tf.nn.relu, kernel_initializer=initializer,\n        bias_initializer=initializer, name='input_layer')\n    dropout_1 = tf.layers.dropout(inputs=layer_1, rate=dropout_rate, training=True)\n    layer_2 = tf.layers.dense(\n        dropout_1, units=512, activation=tf.nn.relu, kernel_initializer=initializer,\n        bias_initializer=initializer, name='disc_layer_1')\n    dropout_2 = tf.layers.dropout(inputs=layer_2, rate=dropout_rate, training=True)\n    layer_3 = tf.layers.dense(\n        dropout_2, units=256, activation=tf.nn.relu, kernel_initializer=initializer,\n        bias_initializer=initializer, name='disc_layer_2')\n    dropout_3 = tf.layers.dropout(inputs=layer_3, rate=dropout_rate, training=True)\n    output_layer = tf.layers.dense(\n        dropout_3, units=1, activation=tf.sigmoid, kernel_initializer=initializer,\n        bias_initializer=initializer, name='disc_output')\n    return output_layer\n</code></pre></p> <p>Now that we have this discriminator defined, let's go ahead and move on to the generator.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#generator-network","title":"Generator network","text":"<p>You can think of the\u00a0<code>generator</code>\u00a0portion of the GAN as a reverse convolutional neural network. Like a VAE, it uses generic normal distribution, the only difference being that it up samples the distribution to form an image. This distribution represents our prior,\u00a0and is updated during training as the GAN improves at producing images that the discriminator is unable to determine whether they are fake.</p> <p>In between each layer, we utilize a\u00a0<code>ReLu</code>\u00a0activation function and\u00a0<code>batch_normalization</code>\u00a0to stabilize each layer's outputs. As the discriminator starts inspecting the outputs of\u00a0<code>generator</code>,\u00a0<code>generator</code>\u00a0will continually adjust the distribution from which it's drawing to closely match the target distribution. The code will look fairly familiar to what you've seen in previous sections: <pre><code>def generator(x, initializer):\n    layer_1 = tf.layers.dense(\n        x, units=256, activation=tf.nn.relu, kernel_initializer=initializer,\n        bias_initializer=initializer, name='input_layer')\n    layer_2 = tf.layers.dense(\n        layer_1, units=512, activation=tf.nn.relu, kernel_initializer=initializer,\n        bias_initializer=initializer, name='hidden_layer_1')\n    layer_3 = tf.layers.dense(\n        layer_2, units=1024, activation=tf.nn.relu, kernel_initializer=initializer,\n        bias_initializer=initializer, name='hidden_layer_2')\n    output_layer = tf.layers.dense(\n        layer_3, units=784, activation=tf.nn.tanh, kernel_initializer=initializer,\n        bias_initializer=initializer, name='generator_output')\n    return output_layer\n</code></pre></p> <p>Now that we have our model set up, let's get into the training process!</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#training-gans","title":"Training GANs","text":"<p>GANs are easy to train, but difficult to optimize due to a number of unstable dynamics in their training processes.\u00a0To train a GAN, we train the generator on sub samples of a high-dimensional training distribution; since this does not innately exist, we initially sample from a standard normal (Gaussian) distribution.</p> <p>!pasted-image-20241231152334.png.md</p> <p>When training GANs, we train to minimize the objective function so that the generator can win. We want the generator to be able to create examples that are realistic enough to fool the discriminator. To do this, we train and optimize the discriminator and the generator in parallel using gradient ascent.\u00a0For each iteration of training, we are going to train the discriminator network in small batches, and then train the generator network in small batches, alternating between the two paradigms. Gradient ascent for the discriminator computes the following: !pasted-image-20241231152415.png.md</p> <p>By taking the maximum of the generator's objective, we're maximizing the likelihood of being wrong. This parallelized training process can still be unstable, however, and stabilizing GANs is a very active area of research at the moment.</p> <p>Let's get back to the TensorFlow process. We'll start by defining our network's training parameters: <pre><code>learning_rate = 0.0002  \nbatch_size = 100  \nepochs = 100  \ndropout_rate=0.5\n</code></pre></p> <p>We then need to define our placeholders, both for the input <code>x</code>, as well as the\u00a0<code>z</code>\u00a0distribution which the generator will generate from: <pre><code>z = tf.placeholder(tf.float32, shape=(None, 100))  \nx = tf.placeholder(tf.float32, shape=(None, 784))\n</code></pre></p> <p>Like before, we'll create a Glorot\u00a0<code>Initializer</code>\u00a0that will initialize our weight and bias values for us: <pre><code>initializer = tf.contrib.layers.xavier_initializer()\n</code></pre></p> <p>Once we have all of this, we can go ahead and actually define our network pieces. You'll notice that for the discriminator, we're using something called a scope. Scopes allow us to reuse items from the TensorFlow graph without generating an error - in this case, we want to use the variables from the discriminator function twice in a row, so we use the\u00a0<code>tf.variable_scope</code>\u00a0function that TensorFlow provides us. Between the two, we simply use the\u00a0<code>scope.reuse_variables()</code>\u00a0function to tell TensorFlow what we're doing: <pre><code>G = generator(z, initializer)  \n\nwith tf.variable_scope('discriminator_scope') as scope:  \n    disc_real = discriminator(x, initializer, 0.5)  \n    scope.reuse_variables()  \n    disc_fake = discriminator(G, initializer, 0.5)\n</code></pre></p> <p>Lastly, we'll define the loss functions for both the generator and discriminator, and set the optimizer: <pre><code>epsilon = 1e-2  \ndisc_loss = tf.reduce_mean(-tf.log(disc_real + epsilon) - tf.log(1 - disc_fake + epsilon))  \ngen_loss = tf.reduce_mean(-tf.log(disc_fake + epsilon))  \n\ndisc_optim = tf.train.AdamOptimizer(lr).minimize(disc_loss)  \ngen_optim = tf.train.AdamOptimizer(lr).minimize(gen_loss)\n</code></pre></p> <p>We can the run the training cycle just as we have in the previous two examples. The only two differences you'll see here is that we run two optimization processes, one for the generator and one for the discriminator: <pre><code>with tf.Session() as sess:  \n    sess.run(tf.global_variables_initializer())   \n    for epoch in range(epochs):  \n\n        ## Define the loss to update as a list  \n        gen_loss = []  \n        disc_loss = []  \n\n        ## Run the training iteration  \n        for iter in range(training_data.shape[0] // batch_size):  \n\n            ## Batch the input for the discriminator  \n            x_prime = training_data[iter*batch_size:(iter+1)*batch_size]  \n            z_prime = np.random.normal(0, 1, (batch_size, 100))  \n\n            ## Run the discriminator session  \n            _, DLoss = sess.run([disc_optim, disc_loss], {x: x_prime, z: z_prime, drop_out: 0.3})  \n            disc_loss.append(DLoss)  \n\n            ## Run the generator session   \n            z_prime = np.random.normal(0, 1, (batch_size, 100))  \n            _, GLoss = sess.run([gen_optim, gen_loss], {z: z_prime, drop_out: 0.3})  \n            gen_loss.append(GLoss)  \n\n        if epoch % 5 == 0:  \n            print('[%d/%d] - loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), epochs, np.mean(D_losses), np.mean(G_losses)))\n</code></pre></p> <p>GANs are fairly computational expensive, so training this network may take a while unless you scale with a web services platform.\u00a0</p> <p>As you can see, all of the models that we've run thus far have built upon each other. Even with advanced generative models like GANs, we can use certain recipes to create powerful neural networks, and larger AI applications, quickly and efficiently.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#other-forms-of-generative-models","title":"Other forms of generative models","text":""},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#overview_2","title":"Overview","text":"<p>While we've only covered two types of generative model, there are many different types that you may encounter in the literature. The following chart is not exhaustive, but does provide a general overview of the types of generative models out there: !pasted-image-20241231152903.png.md</p> <p>Let's break this down:\u00a0 - Explicit density models: Model our data directly from a probability distribution. We explicitly define the probability and solve for it - Implicit density models: Learn to sample from a probability distribution without defining what that distribution is</p> <p>Within explicit density models, we have\u00a0tractable density\u00a0models and\u00a0approximate density\u00a0models. Here, tractable is related to defined computational time; we can calculate the computational complexity of a tractable problem. Approximate density relates to\u00a0intractability\u2014a computer science term that means that there is no defined computational time or algorithm. In practice, an intractable problem utilizes too many computational resources in order to be useful. Therefore, approximate density models use probabilistic approximation techniques to estimate the solution.</p> <p>We'll briefly touch upon three notable classes: fully visible belief nets, Hidden Markov models, and Boltzmann machines. While each of these could be a chapter on its own, we'll touch on them briefly. Examples of each of these networks in Python are available in the code accompanying this chapter.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#fully-visible-belief-nets","title":"Fully visible belief nets","text":"<p>Fully visible belief networks\u00a0are a class of explicit density models\u00a0and a form of\u00a0deep belief network.\u00a0They use\u00a0the chain rule to decompose a probability distribution $p(x)$ over a vector, into a product over each of the members of the vector, represented between by $p(x_i | x_1, \\dots)$. All together, it's formula is: !pasted-image-20241231153111.png.md</p> <p>The most popular model in this family is PixelCNN, an\u00a0autoregressive\u00a0generative model. Pixels approach image generation problems by turning them into a sequence modeling problem, where the next pixel value is determined by all the previously generated pixel values.\u00a0The network scans an image one pixel at a time, and predicts conditional distributions over the possible pixel values.\u00a0We want to assign a probability to every pixel image based on the last pixels that the network saw. For instance, if we're looking at the same horse images as in the previous example, we would be consistently predicting what the next anticipated pixel looks such as follows: !pasted-image-20241231153143.png.md</p> <p>Based on the features that we've seen, will the next pixel still contain the horse's ear, or will it be background? While their training cycles are more stable than GANs, the biggest issue with the networks is that they generate new samples extremely slowly; the model must be run again fully in order to generate a new sample. They also block the execution, meaning that their processes cannot be run in parallel.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#hidden-markov-models","title":"Hidden Markov models","text":"<p>A hidden Markov model is a type of\u00a0Markov model, which is itself a subclass of\u00a0Dynamic Bayesian Networks.\u00a0Markov models are used to model randomly changing systems called\u00a0Markov processes\u00a0also called\u00a0Markov chains. Simply put, a Markov\u00a0process\u00a0is a sequence of events where the probability of an event happening solely depends on the previous event.</p> <p>Markov chains appear as follows: !pasted-image-20241231153404.png.md</p> <p>In this simple chain, there are three states, represented by the circles. We then have probabilities for transitioning to another state, as well as probabilities of staying in a current state. The classic example of a Markov chain is that of the taxi driver, where the driver finds himself currently solely depends on where he was last, in other words, his most recent fare. If we were to apply this example to the preceding Markov chain, the driver would have three possible locations to pick up or drop off customers; the associated probabilities between locations would represent the chance of him going to the other location or staying put.</p> <p>Hidden Markov models are used to model Markov processes that we can't observe; what if the driver's route structure of where he would like to pick up customers is secret? There is likely some logic to the scenario, and we can try and model that process with a Hidden Markov model.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-5-Autoencoders-GANs-DiffusionModels/#boltzmann-machines","title":"Boltzmann machines","text":"<p>Boltzmann machines are a general class of models that contain take\u00a0binary vectors as input and units that assign a probability distribution to each of those binary vectors. As you can see in the following diagram, each unit is dependent on every other unit: !pasted-image-20241231153508.png.md</p> <p>A Boltzmann machine uses something called an\u00a0energy function, which is similar to a loss function. For any given vector,\u00a0the probability of a particular state is proportional to each of the energy function values. To convert this to an actual probability distribution, it's necessary to renormalize the distribution, but this problem becomes another intractable problem. Monte Carlo methods are again used here for sampling as a workaround, hence making Boltzmann machines a Monte Carlo-based method.</p> <p>Let's say we have documents that are represented by binary features. A Boltzmann machine can help us determine whether a particular word or phrase came from a particular document. We can also use Boltzmann machines for anomaly detection in large, complex systems. They work well up to a point, although this method does not work well in high dimensional spaces.</p>"},{"location":"_assets/AI-Book-open-from-MOC/2-6-3Blue1Brown-Explainer/","title":"2.6 3Blue1Brown Explainer","text":"<pre><code>GPT = Generative Pre-trained Transformer\n- **Generative**: Create new text\n- **Pre-trained**: How the model learned\n- **Transformer**: Specific kind of NN, core invention underlying current boom in AI\n</code></pre> <p>\"Attention is all you need\" came from Google in 2017. - Specific use-case was to translate text. - Our use-case is to predict the next word.</p> <pre><code>How data flows through a transformer\n1. **Embedding**\n2. **Attention**\n3. **MLPs**\n4. **Un-embedding**\n</code></pre> <p>Deep Learning describes a class of models that scale very well. (Includes Transformers, MLPs, CNNs, and more.) - The training algorithm is called backpropagation.</p>"},{"location":"_assets/AI-Book-open-from-MOC/5-1-Local-vs-Cloud-Inference/","title":"5.1 Local vs Cloud Inference (Pros & Cons)","text":""},{"location":"_assets/AI-Book-open-from-MOC/5-1-Local-vs-Cloud-Inference/#51-local-vs-cloud-inference-pros-cons","title":"5.1\u202fLocal\u202fvs\u202fCloud Inference\u202f(Pros\u202f&amp;\u202fCons)","text":"<p>Scope Decision\u2011guide for running model inference after training/fine\u2011tuning.</p>"},{"location":"_assets/AI-Book-open-from-MOC/5-1-Local-vs-Cloud-Inference/#key-dimensions","title":"Key Dimensions","text":"Dimension Local (Edge/Laptop) Cloud (AWS/GCP/Azure/HF) Latency Sub\u201150\u202fms possible \u2265\u202f150\u202fms typical Privacy Data stays on\u2011device Data traverses network Cost CapEx once OpEx pay\u2011per\u2011request Scale Limited to device Auto\u2011scales globally Updates Manual scripts CI/CD pipelines Failover Manual (UPS) Multi\u2011zone built\u2011in"},{"location":"_assets/AI-Book-open-from-MOC/5-1-Local-vs-Cloud-Inference/#heuristics","title":"Heuristics","text":"<ul> <li>Choose Local for PII, demos, offline, deterministic batch size.  </li> <li>Choose Cloud for bursty traffic, multi\u2011tenant SaaS, analytics APIs.</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/5-1-Local-vs-Cloud-Inference/#hybrid-patterns","title":"Hybrid Patterns","text":"<ol> <li>Client\u2011side reranking.  </li> <li>On\u2011device fallback.  </li> <li>Federated Learning.</li> </ol>"},{"location":"_assets/AI-Book-open-from-MOC/5-1-Local-vs-Cloud-Inference/#latency-test-snippet","title":"Latency Test Snippet","text":"<pre><code>import time, requests\nt0 = time.time(); _ = model(**inputs); print(f\"local {1000*(time.time()-t0):.1f}\u202fms\")\nprint(\"cloud\", 1000*requests.get(\"https://api.yourmodel.com/ping\").elapsed.total_seconds(), \"ms\")\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/5-2-Quick-Quantization-Laptop-GPU/","title":"5.2 Quick Quantization to Fit on a Laptop GPU","text":""},{"location":"_assets/AI-Book-open-from-MOC/5-2-Quick-Quantization-Laptop-GPU/#52-quick-quantization-to-fit-on-a-laptop-gpu","title":"5.2\u202fQuick Quantization to\u202fFit on a\u202f Laptop\u202fGPU","text":"<p>Shrink FP32/FP16 models to INT8/4\u2011bit so they run in\u00a0\u2264\u202f8\u202fGB VRAM.</p>"},{"location":"_assets/AI-Book-open-from-MOC/5-2-Quick-Quantization-Laptop-GPU/#1-dynamic-int8-pytorch","title":"1. Dynamic INT8 (PyTorch)","text":"<pre><code>from torch.ao.quantization import quantize_dynamic\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nqmodel = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\nqmodel.save_pretrained(\"gpt2\u2011int8\")\n</code></pre> <p>Memory\u00a0cut:\u00a0\u201160\u202f%, speed:\u00a01.4\u00d7.</p>"},{"location":"_assets/AI-Book-open-from-MOC/5-2-Quick-Quantization-Laptop-GPU/#2-4bit-gguf-llamacpp","title":"2. 4\u2011bit GGUF (llama.cpp)","text":"<pre><code>python convert.py --outfile llama7b-q4_0.gguf --wbits 4 --model llama-7b\n./main -m llama7b-q4_0.gguf -p \"Explain quantization\"\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/5-2-Quick-Quantization-Laptop-GPU/#3-bitsandbytes-peft","title":"3. bitsandbytes + PEFT","text":"<pre><code>bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B\", quantization_config=bnb_cfg)\n</code></pre> <p>Gotchas </p> <ul> <li>Calibration data matters for vision; less for text.  </li> <li>Torch &lt;\u202f2.1 may need <code>torch.backends.quantized.engine = 'qnnpack'</code>.</li> </ul>"},{"location":"_assets/AI-Book-open-from-MOC/6-1-Collect-Clean-Text-Data/","title":"6.1 Collect and Clean Text Data","text":""},{"location":"_assets/AI-Book-open-from-MOC/6-1-Collect-Clean-Text-Data/#61-collect-clean-text-data-fast","title":"6.1\u202fCollect\u202f&amp;\u202fClean Text\u00a0Data\u00a0Fast","text":"<p>A 5\u2011step pipeline to build quality corpora.</p> <ol> <li>Source: Common Crawl, arXiv, internal SQL exports.  </li> <li>License/PII filters: <code>scancode-toolkit</code>, <code>presidio</code>.  </li> <li>Cleaning: HTML strip, Unicode normalise, dedup MinHash.  </li> <li>Language filter: <code>langdetect</code> or fastText LID.  </li> <li>Save: Parquet\u00a0+\u00a0Zstd.</li> </ol> <pre><code>from bs4 import BeautifulSoup, Comment\nimport re, langdetect\ndef clean_html(raw):\n    soup = BeautifulSoup(raw, \"lxml\")\n    for c in soup([\"script\",\"style\", Comment]): c.extract()\n    txt = re.sub(r'\\s+', ' ', soup.get_text(' ')).strip()\n    return txt if langdetect.detect(txt) == 'en' else None\n</code></pre>"},{"location":"_assets/AI-Book-open-from-MOC/6-2-CI-CD-for-Models/","title":"6.2 CI CD for Models","text":""},{"location":"_assets/AI-Book-open-from-MOC/6-2-CI-CD-for-Models/#62-cicd-for-models-github-actions","title":"6.2\u00a0CI/CD for\u00a0Models\u00a0(GitHub\u00a0Actions)","text":"<p>Each push \u2192 test \u2192 build Docker \u2192 push \u2192 deploy.</p> <pre><code>name: CI\u2011CD\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with: { lfs: true }\n      - uses: actions/setup-python@v5\n        with: { python-version: '3.11' }\n      - run: pip install -r requirements.txt\n      - run: pytest\n      - run: docker build -t ghcr.io/${{ github.repository }}:${{ github.sha }} .\n      - run: echo $CR_PAT | docker login ghcr.io -u ${{ github.actor }} --password-stdin\n      - run: docker push ghcr.io/${{ github.repository }}:${{ github.sha }}\n</code></pre> <p>Secrets: <code>CR_PAT</code> for GHCR, cloud deploy tokens.</p> <p>Store weights via Git\u2011LFS to avoid huge Docker layers.</p>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/","title":"Supervised 2 Classification","text":"<pre><code>**2025-02-02**: DONE, quality could be improved though.\n</code></pre> <p>Resources - Stanford Cheatsheet - k-nn - 100 days guide</p> <p>Image: Decision Boundaries</p>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#overview","title":"Overview","text":"<ul> <li>Purity Measures: Gauge how homogeneous or \u201cpure\u201d each segment is after splitting.</li> <li>Information Gain: Measures the effectiveness of a new split (or segment) compared to the original.</li> <li>Statistical Significance: Each additional \u201cchild\u201d branch in a tree should be validated, often with a chi-squared test, to confirm that the improvement in segmentation is statistically significant.</li> </ul>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#table-summary","title":"Table Summary","text":"<p>ChatGPT</p> <ul> <li>k-NN is intuitive and simple but can be slow for large datasets.</li> <li>Naive Bayes is fast and works well with high-dimensional data but assumes independent features.</li> <li>Logistic Regression is a well-understood parametric approach, ideal for linearly separable data, and can handle regularization elegantly.</li> <li>Decision Trees are highly interpretable but can overfit if not carefully regularized (via max depth, minimum samples per leaf, etc.).</li> </ul> CATEGORY CORE IDEA MODEL TYPE KEY HYPERPARAMETERS STRENGTHS WEAKNESSES REGULARIZATION k-NN 1. Find k closest neighbors 2. Majority vote or average. Non-parametric k: Number of neighbors.Distance metric Simple to implement. Minimal statistical assumptions. Models complex decision boundaries. Slow for large data. Bad in high-d space. Domain knowledge for good distance metric. Not typically applied, could use dimensionality reduction Naive Bayes Applies Bayes\u2019 Theorem with a naive assumption of conditional independence among features Probabilistic Distribution choice (e.g. Multinomial) Fast Robust with many features. Works well with small datasets. Bad if features are highly correlated. Smoothing techniques to avoid zero probabilities. (e.g., Laplace smoothing) Logistic Regression Estimates the probability of each class via a linear combination of features passed through a sigmoid (or softmax) function, making it a parametric model. Parametric C: Inverse regularization strength. multi_class: One-vs-Rest or multinomial. Regularization type Interpretable coefficients. Can incorporate regularization Good when data is linearly (or log-linearly) separable. May struggle with highly non-linear relationships. Sensitive to outliers if regularization is not used properly. Assumes linear (or log-linear) deci-bound. L1 (Lasso) L2 (Ridge) Elastic Net: Combo of L1 and L2. Decision Trees Splits data into hierarchical branches based on feature values, aiming to maximize \u201cpurity\u201d at each split. Non-parametric max_depth: Maximum tree depth.min_samples_leaf: Minimum samples required in a leaf node. Splitting criterion (e.g., Gini or Entropy). Highly interpretable Handles numeric &amp; categorical Don't need scaling or dummies. Handles multi-output problems. Overfitting if grown without constraints. Greedy splitting may not yield a global optimum. Sensitive to data imbalance. Can struggle with certain complex interactions Indirectly via max_depth, min_samples_leaf, etc. Pruning can reduce overfitting"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#extra-details","title":"Extra Details","text":""},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#naive-bayes","title":"Naive Bayes","text":"<pre><code>Intuition\n- For each observation, given its characteristics (features), the model computes the probability that it belongs to a particular class. (Often relies on different distributions for numeric columns.)\n\n\n**Prob of being in group**\n= prop of my attribute given that they were in the group \n\\* prop of dataset in the group \n/ prop of dataset with my attribute\n\n&lt;img src=\"https://i.imgur.com/pgtG11S.png\" style=\"zoom:33%;\" /&gt;\n</code></pre>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#logistic","title":"Logistic","text":"<pre><code>Target\n- **Binary (Logit)**: Two classes (0 or 1).\n- **Softmax Regression**: Multiple classes (unordered); picks the class with the highest probability.\n- **Ordered Logit**: Multiple _ordered_ classes.\n\n\n$probability(x) \\: = \\: \\frac{1}{e^(-1 \\: * \\: regression \\: model)}$\n\n[Image | Logistic Regression Sigmoid Curve](https://i.imgur.com/nq0l2cu.png)\n\nSteps\n1. For each observation in the raw data, we calculate the sum of the coefficients for all variables. This is f(x).\n2. The x-axis location on the logit plot is the f(x) we calculated above for each observation.\n3. For each observation, we then assign the probability of fitting into the top or bottom group according to the logit curve.\n4. We use the probabilities that we found in the logit model as a way to best fit the line that separates our groups apart to minimize the error in our sample. This is how we determine our original slope and intercept.\n5. We assign L1 as the maximum f(x) that we are willing to allow. (This is lambda)\n   More complex = higher f(x) = larger diamond\n6. We 'underfit' the model until it connects with the blue diamond (ie max complexity we allow). This new model will provide us with a new slope and intercept that will generalize better.\n</code></pre>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#regularization","title":"Regularization","text":"<pre><code>Regularization helps prevent **overfitting** by penalizing large coefficients.\n\n| **Type**                | **Penalty**                                 | **Key Characteristics**                                                                                          |\n|-------------------------|---------------------------------------------|-------------------------------------------------------------------------------------------------------------------|\n| **L1 (Lasso)**          | Sum of absolute values of weights           | - Encourages sparsity (some coefficients may become zero)&lt;br/&gt;- Can be unstable with highly correlated features&lt;br/&gt;- Avoids using all features if many are redundant |\n| **L2 (Ridge)**          | Sum of squared values of weights            | - Tends to shrink coefficients but rarely sets any to zero&lt;br/&gt;- More stable in the presence of correlated features |\n| **Elastic Net (L1 + L2)** | Combination of L1 and L2 penalties         | - Useful when multiple correlated features are suspected&lt;br/&gt;- Retains feature selection from L1 while benefiting from L2\u2019s stability |\n[Image | Regularization](https://i.imgur.com/kkS2s4S.png)\n[Image | Normalization](https://i.imgur.com/sZeYixm.png)\n</code></pre>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#decision-trees","title":"Decision Trees","text":""},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#purity","title":"Purity","text":"<pre><code>When deciding how to split a node, decision tree algorithms use measures like **Gini Impurity** or **Entropy** to assess how \"pure\" the resulting child nodes are. Below is a quick comparison:\n\n| **Measure**      | **Range (Binary Setting)** | **Calculation**                  | **Characteristics**                                                                                  |\n|------------------|----------------------------|----------------------------------|-------------------------------------------------------------------------------------------------------|\n| **Gini Impurity**| 0 (pure) to 0.5 (impure)  | Uses squares of class probabilities | - Slightly faster to compute&lt;br&gt;- Tends to isolate the most frequent class                          |\n| **Entropy**      | 0 (pure) to 0.5 (impure)  | Uses logs of class probabilities    | - Tends to produce more balanced splits                                                              |\n</code></pre>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#week-3-class-eval","title":"Week 3: Class Eval","text":"<ul> <li>How to train a model</li> <li>Classification eval for unbalanced data</li> <li>Sci-kit plot</li> </ul> <p>Accuracy can be misleading. 2 primary reasons: 1. Imbalanced Class Distributions: When one class dominates, accuracy may inflate how well the model performs. 2. Ignoring Economic Costs/Benefits: Different types of errors can have varying costs. It\u2019s often more insightful to build a cost/benefit matrix and maximize profit, rather than maximizing accuracy.</p> <p>Cost-Benefit Approach 1. Construct a \u201ccost/benefit\u201d matrix, detailing the financial impact of each type of prediction:</p> <ul> <li>TP &amp; TN: Represent revenue or benefits.</li> <li>FP &amp; FN: Represent costs or losses.</li> <li>Multiply your confusion matrix by the cost/benefit matrix to calculate expected profit (or cost), and use this to guide decisions.</li> </ul>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#formulas-tp-fp-tn-fn","title":"Formulas (TP, FP, TN, FN)","text":"<pre><code>| **Metric**                            | **Formula**          |\n| ------------------------------------- | -------------------- |\n| **True Positive Rate (TPR) / Recall** | $\\frac{TP}{TP + FN}$ |\n| **False Positive Rate (FPR)**         | $\\frac{FP}{FP + TN}$ |\n| **Precision**                         | $\\frac{TP}{TP + FP}$ |\n| **Recall (Same as TPR)**              | $\\frac{TP}{TP + FN}$ |\n</code></pre>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#model-evaluation-techniques","title":"Model Evaluation Techniques","text":"<p>ChatGPT: Key \"curves\" and model evaluation techniques commonly used in classification:</p> <p><code>Scope</code> - Within: These methods evaluate how well a single model is performing, often helping diagnose issues like overfitting, threshold tuning, and class imbalance. - Across: These methods compare multiple models or evaluate the model\u2019s added value over a random baseline. - Either: Some techniques can be used either within a model (e.g., tuning a threshold) or across models (e.g., selecting the best-performing one).</p> <pre><code>| `Scope` | **Evaluation Technique**                                | **What**                                                                                                              | **Why**                                                                                                                    | **Imbalanced Data Suitability**                                                                                       |                                                                                                     |\n| ------- | ------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n| Within  | **Confusion Matrix**                                    | Shows the counts of (TP, TN, FP, FN)                                                                                  | From here, can derive performance metrics.                                                                                 | -                                                                                                                     | [Imgur](https://i.imgur.com/GqFRVwK.png)                                                            |\n| Within  | **ROC Curve**&lt;br&gt; &lt;br&gt;Receiver Operating Characteristic | Plots TPR vs. FPR at different probability thresholds.                                                                | Offers insight into the trade-off between TP &amp; FP.                                                                         | **Bad**. When negative class is large, the FPR remains deceptively low, which makes ROC curve look overly optimistic. | [Img1](https://i.imgur.com/MWVJ5bc.png), [Img2](https://i.imgur.com/icN6gnX.png)                    |\n| Either  | **AUC**&lt;br&gt;&lt;br&gt;Area Under the ROC Curve                 | A single-number summary (the area under the ROC curve).                                                               | $\\text{AUC} = 1$ indicates a perfect model. &lt;br&gt;&lt;br&gt;$\\text{AUC} = 0.5$ indicates a model with no discriminative power.     | -                                                                                                                     | [Imgur](https://i.imgur.com/HLScrQ3.png)                                                            |\n| Either  | **Precision-Recall Curve**                              | Plots precision vs. recall as the decision threshold varies.                                                          | Especially useful for imbalanced datasets, or when false positives and false negatives incur high costs.                   | **Good**. Focuses on the minority class, where precision and recall are most critical.                                | [Img](https://i.imgur.com/oD7MUiT.png)&lt;br&gt;&lt;br&gt;[Best &amp; Worst Cases](https://i.imgur.com/3ukkvJx.png) |\n| Across  | **Lift Chart**                                          | Compares the model\u2019s performance against a random baseline.                                                           | Shows how many more positives are identified by the model compared to random selection.                                    | **Good**. Especially relevant if you\u2019re trying to identify a small minority class more effectively than chance.       | [Imgur](https://i.imgur.com/F6HmCkn.png)                                                            |\n| Across  | **Gain Chart**                                          | Displays cumulative gain (the fraction of positives identified) as you move through the sorted predictions.           | Similar to Lift, it shows the improvement gained by the model over random selection.                                       | **Good**. Like the Lift chart, it highlights model performance on minority classes.                                   | [Imgur - Profit Curve](https://i.imgur.com/SD9BQdL.png)                                             |\n| Across  | **Cumulative Response Curve**                           | Shows the proportion of positive instances captured as you move through the ranked predictions.                       | Commonly used in marketing and lead-generation applications to understand how quickly you capture most of the \u201cyes\u201d cases. | -                                                                                                                     | [Cumulative response curve](https://i.imgur.com/MJdeBr0.png)                                        |\n| Within  | **Validation Curve**                                    | Plots the training and validation scores across different levels of model complexity (e.g., varying hyperparameters). | Helps diagnose overfitting or underfitting by showing whether the model performance is improving or plateauing.            | -                                                                                                                     | [Imgur](https://i.imgur.com/lnaPwKO.png)                                                            |\n</code></pre>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#single-value-measures","title":"Single-Value Measures","text":"<pre><code>| **Metric**                                 | **What**                                                                                                                                                                          | **Why**                                                                                                                               | **Imbalanced Data Suitability**                                                                              |                                          |\n| ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------- |\n| **F-Measure (F1 Score)**                   | The harmonic mean of precision and recall: &lt;br&gt; $F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$                                    | Combines precision and recall into a single metric, weighting them equally.                                                           | **Good**. Highlights performance on the minority class, where both precision and recall can be low.          | [Imgur](https://i.imgur.com/yV39H3u.png) |\n| **Matthews Correlation Coefficient (MCC)** | A correlation coefficient between observed and predicted classifications: &lt;br&gt; $\\text{MCC} = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$ | Accounts for all four quadrants (\\(TP, TN, FP, FN\\)) and provides a balanced measure even if the classes are of very different sizes. | **Good**. MCC is often more informative than accuracy and works well with imbalanced classes.                | [Imgur](https://i.imgur.com/ekP8auk.png) |\n| **Cohen\u2019s Kappa**                          | Measures agreement between the model\u2019s predictions and the true labels, adjusted for chance agreement.                                                                            | In imbalanced scenarios, a model might appear good by randomly guessing the majority class. Kappa accounts for this chance agreement. | **Mostly good**. While it adjusts for chance, it can still be influenced by highly imbalanced distributions. | [Imgur](https://i.imgur.com/nKydIUS.png) |\n</code></pre> <p>ChatGPT - Types of Means - Arithmetic, harmonic, geometric</p>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-2-Classification/#classification-code","title":"Classification Code","text":"<p>Curves (Matrix, Precision/Recall, ROC)</p> <pre><code># Knn\nparam_grid = dict(n_neighbors = list(range(1,31)), \nweights = [\"uniform\", \"distance\"])\nknn = KNeighborsClassifier()\n\n# Tree\nparam_grid = dict(criterion = [\"gini\", \"entropy\"], \nmax_depth = range(2,10),\nmin_samples_leaf = range(2,8),\nmin_impurity_decrease = [0,1e-8,1e-7,1e-6,1e-5,1e-4])\ngrid_tree_clf = tree.DecisionTreeClassifier(random_state=45)\n\n# Logistic\nparam_grid = dict(penalty = ['l1', 'l2'], \nC = range(1,10))\n\n# SVM\nc = 5# reduce if overfitting\ndegrees = 3\ninfluence = 1 \npoly_kernel_svm_clf = Pipeline([\n (\"scaler\", StandardScaler()),\n (\"svm_clf\", SVC(kernel=\"poly\", degree=degrees, coef0=influence, C=c))\n ])\npoly_kernel_svm_clf.fit(X, y)\n</code></pre> <pre><code># In text\n\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\n\ntarget_names = ['malignant', 'benign']\ny_true = y_test\ny_pred = y_pred\n\nprint(target_names)\nprint(\"Accuracy: {0:.2%}\".format(accuracy_score(y_true, y_pred)))\nprint(\"Precision: {0:.2%}\".format(metrics.precision_score(y_true, y_pred)))\nprint(\"Recall: {0:.2%}\".format(metrics.recall_score(y_true, y_pred)))\nprint(\"F1: {0:.2%}\".format(metrics.f1_score(y_true, y_pred)))\nprint('-------------------------------------')\nprint(classification_report(y_true, y_pred))\n</code></pre> <pre><code># Visually - https://i.imgur.com/PExd8UC.png\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint(classification_report(ytest, yfit, target_names=faces.target_names))\n\nmat = confusion_matrix(ytest, yfit)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\nxticklabels=faces.target_names,\nyticklabels=faces.target_names)\nplt.xlabel('true label')\nplt.ylabel('predicted label');\n</code></pre>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-4-Regression-Eval/","title":"Supervised 4 Regression Eval","text":""},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-4-Regression-Eval/#week-4-regression-eval","title":"Week 4: Regression Eval","text":""},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-4-Regression-Eval/#evaluation","title":"Evaluation","text":"<ul> <li>MAE = Mean absolute error</li> <li>MAPE = Mean absolute pct error</li> <li> <p>RMSE = Root mean squared error   when want to penalize large errors</p> </li> <li> <p>SMAPE = Symmetric mean absolute percentage error   Goes from 0 to 200%   Apply when we are comparing average error of different models   Does not apply when we are looking at each observation</p> </li> </ul>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-4-Regression-Eval/#lift","title":"Lift","text":"<p>Ranked by their predicted number, comparing to the average</p> <p>Y-axis = Revenue</p> <p>Image</p>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-6-Ensemble/","title":"Supervised 6 Ensemble","text":""},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-6-Ensemble/#week-6-ensemble","title":"Week 6: Ensemble","text":"<ul> <li>Interactive Playground</li> <li> <p>Short Review Video</p> </li> <li> <p>Bias vs Variance Graph</p> </li> <li>Summary of 6 techniques</li> </ul> <p>Data vs Model</p> <ul> <li>Data centric: boosting and bagging (different training data)</li> <li>Model centric: stacking (different training algorithms)</li> </ul> <p>Why ensemble?</p> <ol> <li>Inherent randomness in the world</li> <li>Bias: when no matter how many obs we get, our model will never reach perfection</li> <li>Variance: we are limited to samples, and these samples are never identical</li> </ol> <p>Bias/Variance Tradeoff</p> <p>Ensemble models: when we have high accuracy &amp; high variance, can average them out to bring the variance down while still retaining the high accuracy</p>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-6-Ensemble/#using-different-algorithms","title":"Using different algorithms","text":"<ul> <li>Hard vote: majority vote of multiple models</li> <li>Soft vote (preferred): highest avg probability (more weight to more confidence)</li> <li>Works best when each initial algo is a weak learner</li> </ul>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-6-Ensemble/#bagging-and-pasting","title":"Bagging and Pasting","text":"<ul> <li>Use the same algorithm, but train on different subsets of the training data (at the same time).</li> <li>Usually gives similar bias, but smaller variance</li> <li>Works well when each run is making mistakes on different observations</li> </ul> <p>Bag vs Paste:</p> <ul> <li>Bagging: With replacement</li> <li>Bagging is higher bias, lower variance. Usually performs better, but should use cv to check</li> <li>Pasting: Without replacement</li> </ul> <p>Random patches sampling rows and columns Random subspaces sampling columns (<code>boostrap=False, max_samples=1, bootstrapfeatures=True, max_features&lt;1</code>)</p> <p>This reduces variance because its comparing averages, not just one answer. Helps keep accuracy high for both in-sample and out-of-sample</p> <p>Hyperparameters</p> <ul> <li>feature: max_features</li> <li>feature: bootstrap_features</li> <li>instance: max_samples (normally set to size of training set)</li> <li>instance: bootstrap</li> </ul> <p>Random Forest</p> <p>Randomness: We are able to keep the full tree, not pruned</p> <ul> <li>Data: Different random sample</li> <li>Features: For each tree, selects best feature to split on from a random subset of features.</li> </ul> <p>Extremely randomized trees also uses random thresholds for each feature when splitting</p>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-6-Ensemble/#boosting","title":"Boosting","text":"<p>Image: Ada vs Gradient Boosting</p> <p>XGBoost Pydata Vid</p> <p>Review pg 205 for hyperparameters</p> <ul> <li>Run model</li> <li>Get residuals</li> <li>Train another model on the residuals</li> <li>Repeat</li> <li>Final prediction: Sum up predictions from each model</li> </ul> <p>Shrinkage: small <code>learning_rate</code> + more trees</p>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-6-Ensemble/#adaboost","title":"AdaBoost","text":"<ol> <li>Run model</li> <li>Add weight to observations we got wrong (<code>learning_rate</code> to tell how much to learn)</li> <li>Train another model using the updated weights (original data, not the residuals)</li> <li>Repeat</li> <li>Final prediction: Each of the models makes a prediction, weighted vote based on accuracy on the weighted training set</li> </ol> <p>We could do <code>bootstrap=True</code>, but the final vote is still based on accuracy on training set</p>"},{"location":"_assets/ML-Deeper-open-from-MOC/Supervised-6-Ensemble/#stacking","title":"Stacking","text":"<ol> <li>Split the training set into 2 subsets (if we are only doing the blender once)    $\\quad$ 1st to train original classifer runs    $\\quad$ 2nd is the <code>holdout</code>: use the runs from above to make predictions on this set</li> <li>If we had 3 runs of the classifier, we will now have 3 predicted values for each row in the holdout set.</li> <li>Create a new training set using the 3 predicted values (as features) for each obs + the 1 true value (target)</li> <li>Train the <code>blender</code> on this new training set and make predictions using this model</li> </ol>"},{"location":"_assets/Templates/Template/","title":"Template","text":"<p>CoverImage: null Covers: null Due: null Function: Hierarchy HoursDone: 0 HoursRemain: 0 Objective: Reference Quality: \u2605 QualityComment: Why isn't this a 10? ReviewFreq: Weekly, 1-Month, 2-Month, 3-Month TimeSpent: null TimeSpent2: null _kMDItemDisplayNameWithExtensions: Template.md ai_abstract: null ai_key_terms: [] aliases: null children: 0 created: '2025-07-25' cssclasses: null grandchildren: 0 kMDItemContentCreationDate: 2024-12-26 21:02:50 +0000 kMDItemContentCreationDate_Ranking: 2025-02-01 00:00:00 +0000 kMDItemContentModificationDate: 2024-12-28 22:47:21 +0000 kMDItemContentType: net.daringfireball.markdown kMDItemContentTypeTree: ( kMDItemDateAdded: 2025-02-01 17:16:38 +0000 kMDItemDocumentIdentifier: '97125' kMDItemFSCreatorCode: '' kMDItemFSFinderFlags: '0' kMDItemFSHasCustomIcon: (null) kMDItemFSInvisible: '0' kMDItemFSIsExtensionHidden: '0' kMDItemFSIsStationery: (null) kMDItemFSLabel: '0' kMDItemFSNodeCount: (null) kMDItemFSOwnerGroupID: '20' kMDItemFSOwnerUserID: '502' kMDItemFSTypeCode: '' kMDItemInterestingDate_Ranking: 2024-12-27 00:00:00 +0000 kMDItemLastUsedDate: 2024-12-27 16:51:24 +0000 kMDItemLastUsedDate_Ranking: 2024-12-27 00:00:00 +0000 kMDItemUseCount: '4' kMDItemUsedDates: ( modified: '2024-12-28' published: true reading_time: 0.0 source_file: Template.md tags: null title: Template word_count: 0</p>"},{"location":"_assets/TimeSeries/Theory-Univariate/","title":"Theory   Univariate","text":"<ul> <li>Springer Textbook</li> <li>University of South Carolina Lecture Notes (Heavy)</li> <li>ChatGPT for cleaning this note doc</li> </ul>"},{"location":"_assets/TimeSeries/Theory-Univariate/#1-intro","title":"1. Intro","text":"Ch Topic Topic 2 Basics Mean, covariance, correlation, stationarity 3 Trend How to estimate &amp; check common deterministic trend models 4 ARMA Stationary, aka Box-Jenkins models 5 ARIMA Nonstationary 6 Heart Techniques for tentatively specifying models 7 Heart Efficiently estimating the model parameters using least squares and maximum likelihood 8 Heart Determining how well the models fit the data 9 MSE Theory &amp; methods of MSE for ARIMA - - The remaining chapters cover selected topics and are of a somewhat more advanced nature. <pre><code>**Process**:\n1. **Check stationarity**.\n2. **If nonstationary**, use differencing or other transformations.\n3. Identify the number of days to use for ARMA using ACF, PACF, EACF.\n4. **Estimate model parameters**.\n5. **Validate model** (fit, residual checks, etc.).\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#2-cheatsheet","title":"2. Cheatsheet","text":"<pre><code>**Basics**\n- **Mean**: Average value over time.\n- **Covariance &amp; Correlation**: Measure how two time points move together.\n- **Stationarity**: Means, variances, and autocovariances do not change over time.\n\n\n**Forecasting**: when consecutive observations are **not** independent. \n- *autocorrelation*: correlation between time points\n- *white noise*: no correlation\n</code></pre> <pre><code>1. **Stationarity**\n   - **Definition**: Constant mean, constant variance, and autocovariance depends only on lag.  \n   - **Check**: Plot data, use ADF test, consider differencing if nonstationary.\n&lt;br&gt;\n1. **Autocorrelation Functions**  \n   - **ACF**: Helps identify MA processes (autocorrelations that cut off abruptly often indicate MA).  \n   - **PACF**: Helps identify AR processes (partial autocorrelations that cut off abruptly often indicate AR).\n&lt;br&gt;\n1. **Model Families**  \n   - **AR(p)**: Depends on its own past **values**.  \n   - **MA(q)**: Depends on past forecast **errors**.  \n   - **ARMA(p, q)**: Combination of AR and MA (stationary).  \n   - **ARIMA(p, d, q)**: Adds differencing for nonstationary data.  \n   - **SARIMA**: Adds seasonal components.\n&lt;br&gt;\n1. **Model Selection**  \n   - **ACF/PACF/EACF** to guess initial p, q.  \n   - **AIC/BIC**: Compare candidate models; lower is better.  \n   - Residual checks to ensure white-noise residuals.\n&lt;br&gt;\n1. **Forecasting**  \n   - **Naive**: Use the last observed value.  \n   - **Averaging**: Use the mean of recent observations.  \n   - **Exponential Smoothing**: Heavier weight on recent observations.  \n   - **ARIMA-based**: Incorporates AR/MA terms and differencing.\n&lt;br&gt;\n1. **Trends &amp; Seasonality**  \n   - **Deterministic Trend**: Model explicitly if present (linear, polynomial, etc.).  \n   - **Seasonality**: SARIMA or explicit seasonal terms.\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#3-stationary-vs-nonstationary","title":"3. Stationary vs. Nonstationary","text":"<pre><code>**Stochastic Process**: A sequence of RVs indexed by time.\n\n- **Sample Path**: One particular realization of that stochastic process.\n- **Stationarity**:\n\n   - Constant mean over time.\n   - Constant variance.\n   - Constant autocovariance (depends only on the lag).\n   - No inherent seasonality.\n\n**Why Stationarity?**  \nWith only one observed path, stationarity lets us make reliable inferences about the underlying process from that single path.\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#random-walk-nonstationary","title":"Random Walk (Nonstationary)","text":"<pre><code>- Values evolve via accumulating errors over time.\n- Variance grows with time.\n- Apparent \u201ctrend\u201d might be random fluctuation.\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#converting-to-stationary","title":"Converting to Stationary","text":"<pre><code>1. **1st differencing**:  \n\n   - A random walk $Y_t$ becomes stationary if you take $Y_t - Y_{t-1}$.  \n   - Use the **ADF test** to decide if differencing is needed.\n1. **2nd differencing** if one differencing step is not enough.  \n2. **Log transform** if variance grows with the level of the series (common for financial data).\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#4-model-classes","title":"4. Model Classes","text":""},{"location":"_assets/TimeSeries/Theory-Univariate/#41-white-noise","title":"4.1 White Noise","text":"<pre><code>- **Definition**: Sequence of i.i.d. RVs with mean 0 and constant variance.\n- **Autocorrelation**: Zero at all lags.\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#42-moving-average-ma","title":"4.2 Moving Average (MA)","text":"<pre><code>- **MA(q)**: Current value depends on past $q$ errors (white-noise terms).\n- Example: **MA(2)**\n\n  $X_{t} = \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t-1} + \\theta_{2}\\varepsilon_{t-2}$\n\n   - **Expected value** of $X_t$ is 0 (if no constant term).\n   - **Variance** of $X_t$ is $1 + \\theta_{1}^2 + \\theta_{2}^2$ (assuming $\\varepsilon_t \\sim \\text{iid}(0,1)$).\n   - **Covariance** terms depend on $\\theta_{i}$ values and the lag.\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#43-autoregressive-ar","title":"4.3 Autoregressive (AR)","text":"<pre><code>- **AR(p)**: Current value depends on its own past $p$ values.\n- Example: **AR(1)**\n\n  $X_{t} = \\phi X_{t-1} + \\varepsilon_{t}$\n\n   - Stationary if $|\\phi| &lt; 1$.\n   - **Variance** of $X_t$ for AR(1):  \n    $\\text{Var}(X_t) = \\frac{1}{1 - \\phi^2}$\n   - **Covariance** at lag 1:  \n    $\\text{Cov}(X_t, X_{t-1}) = \\frac{\\phi}{1 - \\phi^2}$\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#backshift-notation","title":"Backshift Notation","text":"<pre><code>- **Backshift operator** $B$: $B(X_{t}) = X_{t-1}$.\n- **AR(1) in backshift form**:\n\n  $(1 - \\phi B)X_t = \\varepsilon_t$\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#44-arma","title":"4.4 ARMA","text":"<pre><code>- **ARMA(p, q)** = Autoregressive part (p) + Moving Average part (q).\n- **Stationarity**: Required for ARMA to work properly.\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#45-arima","title":"4.5 ARIMA","text":"<pre><code>- **ARIMA(p, d, q)**: Same as ARMA but the series is differenced $d$ times to achieve stationarity.\n- In backshift form:\n\n  $(1 - B)^d X_t \\quad \\text{follows an ARMA}(p,q)$\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#46-sarima","title":"4.6 SARIMA","text":"<pre><code>- Adds **seasonal** terms for both autoregressive and moving-average, as well as seasonal differencing.\n- Notation: **SARIMA$(p,d,q)(P,D,Q)_m$** where $m$ is the seasonal period (e.g., 12 for monthly data with yearly seasonality).\n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#6-forecasting-methods","title":"6. Forecasting Methods","text":"<pre><code>1. **Naive**  \n   - Forecast is simply the last observed value.\n\n2. **Average**  \n   - Forecast is the mean of recent or all observed values.\n\n3. **Exponential Smoothing**  \n   - Weighted average of past observations where weights decay exponentially.\n\n4. **ARIMA-based Forecasts**  \n   - Use the fitted ARIMA model to predict future values, taking into account AR/MA terms and differencing.\n\n**Example**:  \n- **Simple Exponential Smoothing** $\\approx$ ARIMA$(0,1,1)$ under some parameter relationships.  \n</code></pre>"},{"location":"_assets/TimeSeries/Theory-Univariate/#7-trends-seasonality","title":"7. Trends &amp; Seasonality","text":"<pre><code>- **Deterministic Trend**:  \n\n   - A function of time (linear, polynomial).  \n   - $Y_t = f(t) + \\text{stationary noise}$.\n   - If trend is linear ($f(t) = \\beta_0 + \\beta_1 t$), differencing can remove the linear component.\n\n- **Seasonality**:  \n\n   - Patterns repeat at fixed intervals.  \n   - Handle with seasonal differencing or adding seasonal AR/MA terms (SARIMA).\n\n- **Tests for Trend**:  \n\n   - **ADF**: If p-value is high, the series might need differencing or might have a deterministic trend.  \n   - **Residual Analysis**: Check whether residuals are white noise. If not, the trend model might be inadequate.\n</code></pre>"},{"location":"_assets/TimeSeries/Video-Series/","title":"Video Series","text":"<ul> <li>Udacity: Time Series Forecasting w TensorFlow (Free)</li> </ul>"},{"location":"_assets/TimeSeries/Video-Series/#rnn","title":"RNN","text":"<p>TensorFlow Guide</p> <p>RNN's are networks of repeating modules, each passing a message to a successor and allowing information to persist.</p> <p>Cell state: Horizontal top line. Updated by gates.</p> <p>4 layers (yellow) - Forget gate: Remove from the cell state - Input gate: Values to update from previous module - Tanh: Apply to step 2, add to the cell state - Sigmoid: Output to next module</p> <p></p> <p>14. Video</p> <p>Process of RNN (RNN: Contains recurrent layers) (Image)</p> <ol> <li>Take in the 3D input windows</li> <li>Batch size</li> <li>## of time steps</li> <li>## of features in the model</li> <li>Send to a Recurrent Layer, composed of a single memory cell</li> <li>Take value from previous time step</li> <li>Output value for current time step AND the state/context so the model runs sequentially</li> <li>Repeat</li> <li>Repeat #2</li> <li>Output forecast (ie Sequence to Vector)</li> </ol>"},{"location":"_assets/TimeSeries/Video-Series/#lectures","title":"Lectures","text":""},{"location":"_assets/TimeSeries/Video-Series/#0-basics-overview","title":"0. Basics Overview","text":"<ul> <li>4. Common patterns: White noise, trend, seasonality</li> <li>6. Forecasting: Naive forecast, fixed vs roll forward partitioning</li> <li>8. Metrics: Differencing, MA, smoothing</li> <li>10. Time Windows</li> </ul> <p>Steps: 1. Tuning: Train on training data, test on validation data 2. Estimating production: Train on training &amp; validation data, test on test data 3. Production: Train on all 3, predict out</p>"},{"location":"_assets/TimeSeries/Video-Series/#01-pre-steps","title":"01. Pre-Steps","text":"<p>We want to make the time series as simple as possible before sending it to the model.</p> <p>Need to get rid of the following: - Trend - Seasonality (months, weekdays, etc)    - Make sure train-val-test captures this seasonality</p> <p>Use roll-forward partitioning instead of fixed partitioning  (Video) - Fixed: Normal - Roll forward: Start with a short training period and then predict out. (Essentially mimicking real-life).   Note: Takes much longer</p> <p>Metrics video - Differencing: This helps get rid of the trend &amp; seasonality - MA: Eliminates some noise but does not anticipate trend &amp; seasonality (apply differencing first) - Forecast for both = trailing MA of differencing TS + centered MA of past series (t-365)</p> <pre><code>import pandas as pd\nseries = pd.Series(series)\n\nsplit_time = 1000    ### Train vs test\nts_diff = 365        ### Number of time periods to use for differencing\nts_ma = 50           ### Number of time periods to use for moving average\nts_smooth_past = 11\nts_smooth_begin = ts_diff + np.floor(ts_smooth_past / 2)\nts_smooth_end = ts_diff - np.ceil(ts_smooth_past / 2)\n\n### Differencing\ndiff_series = series.diff(ts_diff).dropna()\n\n### MA\ndiff_moving_avg = diff_series.rolling(ts_ma, closed='left').mean().dropna().iloc[split_time - ts_diff - ts_ma:]\ndiff_moving_avg_plus_past = (diff_moving_avg + series.shift(ts_diff)).dropna()\n\n### Both\nsmoothed = series.rolling(ts_smooth_past, closed='left').mean().dropna().iloc[split_time - int(ts_smooth_begin):-int(ts_smooth_end)]\ndiff_moving_avg_plus_smooth_past = smoothed + diff_moving_avg.values\n</code></pre>"},{"location":"_assets/TimeSeries/Video-Series/#04-windowing","title":"04. Windowing","text":"<p>The main features of the input windows are: - The width (number of time steps) of the input and label windows. - The time offset between them. - Which features are used as inputs, labels, or both.</p> <p>Example: Take 24 hours and give a prediction 24 hours in the future. - Input width = 24 - Offset = 24 - Total width = 48 - Label width = 1</p> <p>Intro to Tensors - Tensor: Think of them as np.array that can be 1D, 2D, 3D, etc.    - Can be 1 column or more, need to be the same dtype. Basically an np.array. - Element: Each value in a tensor. Could be nested which would then contain multiple components</p>"},{"location":"_assets/TimeSeries/Video-Series/#05-ml","title":"05. ML","text":"<p>Video: 12. Forecasting with ML</p> <p>Sample, Batch, Epoch - Sample: one element of a dataset. (One row) - Batch: a set of N samples. The larger the batch, the better the approximation; pick as large as you can afford without running out of memory - Epoch: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.</p> <p>SGD with some momentum helps converge quickly. Could try Adam as well.</p> <p>Huber Loss for training: Good for optimizing MAE - quadratic for small errors (MSE) - linear for large errors (MAE)</p> <p>Early Stopping Callback: - Patience = 10  --&gt; Interupts training when validation doesn't improve for 10 consecutive epochs - This allows us to set epochs = 500 because early stopping will happen way sooner</p>"},{"location":"_assets/TimeSeries/Video-Series/#things-to-be-aware-of","title":"# Things to be aware of","text":"<p>Video 1 1. Do I have the right number of neurons? 2. Do I have the right number of layers? 3. Learning rate too..    1. High: Training will be unstable, model won't learn    2. Low: Training will be slow 4. Do I have early stopping set right? Loss can jump up/down unpredictably during training.</p> <p>Video 1. Vanishing gradient: This often occurs when back propagating through many layers / time steps, especially when detecting long term patterns. 2. 1. 1 Approach: Make a prediction at each step time (ie Sequence to Sequence). Function: <code>seq2seq_window_dataset</code> 3. RNNs are useful when we have lots of high-frequency data and the signal:noise ratio is high</p> <p>Gradient update: $$\\text{New weight} = Weight - LR \\: * \\: Gradient$$</p> <p>During backpropagation, RNNs suffer from vanishing gradient. When going from start to finish, the updates will be too small and the network won't learn.</p> <p>LSTM uses gates to throw away unnecessary info and only keep meaningful.</p> <p>Within one cell: 1. New vector: Combine hidden state (ie prior info) and current input 2. New hidden state: Apply tan transformation to step 1    1. Note: tan to keep regularized between -1 and 1</p>"}]}