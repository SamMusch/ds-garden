---
published: true
---

## 3.3 Data conversion (embeddings)

load --> split --> *embed* --> store

- store chunks as **embeddings** --> store in *vector database*

**embeddings | motivating example**: 2 ways to find info

- keywords --> match docs --> show results

- semantics --> match **embeddings** --> show results

**embeddings**

- **are**: vector representations of data (words, sentences, etc)

- **are**: data transformed into *n*-dimensional matrices

- **do**: calc similarity & establish semantic relationships

**embedding models** (for w/s/p)

- **purpose**: Enable similarity search. Position similar w/s/p near each other.

- **how**: convert w/s/p into *n*-dim ***vectors***

**embeddings** use cases:

- *Text search (RAG)*: search KB for optimal chunk

- *Clustering*: categorize similar data together

- *ML*: convert text --> numbers (features)

**embedding algos** | considerations:

- ***Use case***: select based on your task (eg retrieval, semantic text similarity, summarization)

- ***Cost***: more tokens --> more dollars

[HF MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

| Embedding algos                                                                              | Team            | Note                                           |
| -------------------------------------------------------------------------------------------- | --------------- | ---------------------------------------------- |
| *Word2Vec*                                                                                   | Google          | shallow NN                                     |
| *<abbr title="Global Vectors for Word Representations">GloVe</abbr>*                         | Stanford        | unsupervised learning                          |
| *FastText*                                                                                   | Meta            | shallow NN, extends Word2Vec                   |
| *<abbr title="Embeddings from Language Models">ELMo</abbr>*                                  | Allen Institute | for Q&A and sentiment                          |
| *<abbr title="Bidirectional Encoder Representations from Transformers">BERT</abbr>* (Transf) | Google          | provides contextualized word embeddings via bi |

OpenAI's:

- *ada-002* (2022-12) 1536 dims

- *3-small* (2024-01) 1536 dims. Users can adjust size based on their needs.

- *3-large* (2024-01) 3072 dims.

#### Extra Notes

*vector*:

- **in physics**: an object with magnitude (length) & direction

- **in ML**: an abstract representation of data (array or list, rep a feature/attribute)

- **in NLP**: can rep a doc, a sentence, a word.

embedding example:

- **words**: dog, bark, fly.

- **similarities** (2D):

    - **contextually**: dog & bark

    - **grammatically**: bark & fly (verbs)

**SIMILARITY**
Similar pieces of text lie close to each other.
similarity calculations | common measures

- *cosine similarity*: use **angles**. (0 deg = similar, 90 deg = unrelated, 180 deg = opposite)

- *euclidean distance*: use **distance**