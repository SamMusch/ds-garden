---
CoverImage: null
Covers: null
Due: null
Function: null
HoursDone: null
HoursRemain: null
Objective: null
Quality: null
QualityComment: null
ReviewFreq: null
TimeSpent: null
TimeSpent2: null
_kMDItemDisplayNameWithExtensions: 3.3-Converting-Embedding.md
ai_abstract: null
ai_key_terms: []
aliases: null
children: 0
created: '2025-10-04'
cssclasses: null
grandchildren: 0
kMDItemContentCreationDate: 2025-08-11 19:43:04 +0000
kMDItemContentCreationDate_Ranking: 2025-08-11 00:00:00 +0000
kMDItemContentModificationDate: 2025-10-04 15:51:55 +0000
kMDItemContentType: net.daringfireball.markdown
kMDItemContentTypeTree: (
kMDItemDateAdded: 2025-08-11 19:46:12 +0000
kMDItemDocumentIdentifier: '176621'
kMDItemFSCreatorCode: ''
kMDItemFSFinderFlags: '0'
kMDItemFSHasCustomIcon: (null)
kMDItemFSInvisible: '0'
kMDItemFSIsExtensionHidden: '0'
kMDItemFSIsStationery: (null)
kMDItemFSLabel: '0'
kMDItemFSNodeCount: (null)
kMDItemFSOwnerGroupID: '20'
kMDItemFSOwnerUserID: '502'
kMDItemFSTypeCode: ''
kMDItemInterestingDate_Ranking: 2025-10-04 00:00:00 +0000
modified: '2025-10-04'
published: true
reading_time: 1.5
source_file: 3.3-Converting-Embedding.md
tags: null
title: 3.3 Converting Embedding
word_count: 302
---

## 3.3 Data conversion (embeddings)

load --> split --> *embed* --> store

- store chunks as **embeddings** --> store in *vector database*

**embeddings | motivating example**: 2 ways to find info

- keywords --> match docs --> show results

- semantics --> match **embeddings** --> show results

**embeddings**

- **are**: vector representations of data (words, sentences, etc)

- **are**: data transformed into *n*-dimensional matrices

- **do**: calc similarity & establish semantic relationships

**embedding models** (for w/s/p)

- **purpose**: Enable similarity search. Position similar w/s/p near each other.

- **how**: convert w/s/p into *n*-dim ***vectors***

**embeddings** use cases:

- *Text search (RAG)*: search KB for optimal chunk

- *Clustering*: categorize similar data together

- *ML*: convert text --> numbers (features)

**embedding algos** | considerations:

- ***Use case***: select based on your task (eg retrieval, semantic text similarity, summarization)

- ***Cost***: more tokens --> more dollars

[HF MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

| Embedding algos                                                                              | Team            | Note                                           |
| -------------------------------------------------------------------------------------------- | --------------- | ---------------------------------------------- |
| *Word2Vec*                                                                                   | Google          | shallow NN                                     |
| *<abbr title="Global Vectors for Word Representations">GloVe</abbr>*                         | Stanford        | unsupervised learning                          |
| *FastText*                                                                                   | Meta            | shallow NN, extends Word2Vec                   |
| *<abbr title="Embeddings from Language Models">ELMo</abbr>*                                  | Allen Institute | for Q&A and sentiment                          |
| *<abbr title="Bidirectional Encoder Representations from Transformers">BERT</abbr>* (Transf) | Google          | provides contextualized word embeddings via bi |

OpenAI's:

- *ada-002* (2022-12) 1536 dims

- *3-small* (2024-01) 1536 dims. Users can adjust size based on their needs.

- *3-large* (2024-01) 3072 dims.

#### Extra Notes

*vector*:

- **in physics**: an object with magnitude (length) & direction

- **in ML**: an abstract representation of data (array or list, rep a feature/attribute)

- **in NLP**: can rep a doc, a sentence, a word.

embedding example:

- **words**: dog, bark, fly.

- **similarities** (2D):

    - **contextually**: dog & bark

    - **grammatically**: bark & fly (verbs)

**SIMILARITY**
Similar pieces of text lie close to each other.
similarity calculations | common measures

- *cosine similarity*: use **angles**. (0 deg = similar, 90 deg = unrelated, 180 deg = opposite)

- *euclidean distance*: use **distance**