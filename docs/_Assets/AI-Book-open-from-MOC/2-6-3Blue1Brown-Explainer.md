---
CoverImage: null
Covers: null
Due: null
Function: null
HoursDone: null
HoursRemain: null
Objective: null
Quality: null
QualityComment: null
ReviewFreq: null
TimeSpent: null
TimeSpent2: null
_kMDItemDisplayNameWithExtensions: 2.6 3Blue1Brown Explainer.md
ai_abstract: null
ai_key_terms: []
aliases: null
children: 0
created: '2025-07-18'
cssclasses: null
grandchildren: 0
kMDItemContentCreationDate: 2025-04-20 14:21:50 +0000
kMDItemContentCreationDate_Ranking: 2025-05-20 00:00:00 +0000
kMDItemContentModificationDate: 2025-04-20 15:33:14 +0000
kMDItemContentType: net.daringfireball.markdown
kMDItemContentTypeTree: (
kMDItemDateAdded: 2025-05-20 16:34:46 +0000
kMDItemDocumentIdentifier: '100722'
kMDItemFSCreatorCode: ''
kMDItemFSFinderFlags: '0'
kMDItemFSHasCustomIcon: (null)
kMDItemFSInvisible: '0'
kMDItemFSIsExtensionHidden: '0'
kMDItemFSIsStationery: (null)
kMDItemFSLabel: '0'
kMDItemFSNodeCount: (null)
kMDItemFSOwnerGroupID: '20'
kMDItemFSOwnerUserID: '502'
kMDItemFSTypeCode: ''
kMDItemInterestingDate_Ranking: 2025-04-20 00:00:00 +0000
modified: '2025-04-20'
published: true
reading_time: 0.5
source_file: 2.6 3Blue1Brown Explainer.md
tags: null
title: 2.6 3Blue1Brown Explainer
word_count: 95
---

!!! note
    GPT = Generative Pre-trained Transformer
    - **Generative**: Create new text
    - **Pre-trained**: How the model learned
    - **Transformer**: Specific kind of NN, core invention underlying current boom in AI


"Attention is all you need" came from Google in 2017.
- Specific use-case was to translate text.
- Our use-case is to predict the next word.

!!! note
    How data flows through a transformer
    1. **Embedding**
    2. **Attention**
    3. **MLPs**
    4. **Un-embedding**


Deep Learning describes a *class* of models that scale very well. (Includes **Transformers**, MLPs, CNNs, and more.)
- The *training* algorithm is called backpropagation.