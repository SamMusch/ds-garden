---
CoverImage: null
Covers: ML Foundations
Due: null
Function: Network
HoursDone: 50
HoursRemain: 0
Objective: Reference
Quality: ★★★★
QualityComment: Basics
ReviewFreq: 1-Month
TimeSpent: null
TimeSpent2: null
_kMDItemDisplayNameWithExtensions: ML.md
ai_abstract: null
ai_key_terms: null
aliases: null
children: 0
created: 2025-07-18
cssclasses: null
grandchildren: 0
kMDItemAlternateNames: (
kMDItemContentCreationDate: 2024-09-06 16:49:35 +0000
kMDItemContentCreationDate_Ranking: 2024-09-06 00:00:00 +0000
kMDItemContentModificationDate: 2025-05-18 14:52:44 +0000
kMDItemContentType: net.daringfireball.markdown
kMDItemContentTypeTree: (
kMDItemDateAdded: 2025-02-01 17:16:38 +0000
kMDItemDocumentIdentifier: '97063'
kMDItemFSCreatorCode: ''
kMDItemFSFinderFlags: '16'
kMDItemFSHasCustomIcon: (null)
kMDItemFSInvisible: '0'
kMDItemFSIsExtensionHidden: '1'
kMDItemFSIsStationery: (null)
kMDItemFSLabel: '0'
kMDItemFSNodeCount: (null)
kMDItemFSOwnerGroupID: '20'
kMDItemFSOwnerUserID: '502'
kMDItemFSTypeCode: ''
kMDItemInterestingDate_Ranking: 2024-12-29 00:00:00 +0000
kMDItemLastUsedDate: 2024-12-29 20:07:59 +0000
kMDItemLastUsedDate_Ranking: 2024-12-29 00:00:00 +0000
kMDItemUseCount: '12'
kMDItemUsedDates: (
kMDItemUserCreatedDate: (
kMDItemUserCreatedUserHandle: (
modified: 2025-05-18
published: true
reading_time: 7.8
source_file: ML.md
tags:

- ml_

- supervised
title: ML
word_count: 1570
---

## Deeper Links

| Notes                    | Contains                          |
| ------------------------ | --------------------------------- |
| [[ML-01-Classification]] | Models, evaluation, and some code |
| [[ML-02-Regression]]     | Evaluation                        |
| [[ML-03-Ensemble]]       | Models                            |
| [[ML-04-DL]]             |                                   |

>[!sam]
>ML components ([Source](https://learning.oreilly.com/library/view/hands-on-artificial-intelligence/9781788991063/c72aa49d-41f1-4a15-bee5-9efc9190f282.xhtml))
>- dataset
>- model
>- loss function
>- optimization technique

---

Use ML when:

- Complex problems

- Lots of data

- Fluctuating environments

### Data Mining Tasks

1. **Classification**

2. **Regression**

3. **Similarity matching**: X bought from us. Who else is likely to?

4. **Clustering**

5. **Co-occurence** (market basket analysis)

6. **Profiling**: What is the typical behavior of this segment?

7. **Link prediction**: You and x share 10 friends. She likes this person, so you prob will too.

8. **Data reduction**: Dropping unnecessary info

9. **Causal modeling**: What influences our DV?

### Data Mining Process
ML lifecycle: steps for transforming data --> actionable insights

1. **Business Understanding**: Define the problem & success criteria.

2. **Data Understanding**: How was it collected? Any implicit biases?

3. **Data Preparation**

4. **Model**

    1. **Data Preparation** (possibly)

    2. **Modeling** (possibly)

5. **Evaluate**

6. **Deploy**

---
## Types of Systems


!!! sam
    Broad categories are based on:

    - Are they trained with human supervision? 

        - **Supervised**: Where data includes labels for learning.

        - **Unsupervised**: Where data lacks explicit labels.

        - **Semisupervised**: 

        - **Reinforcement**: Where an agent learns by interacting with an environment.

    - Can they learn incrementally on the fly?

        - **Online**: Yes

        - **Batch**: No

    - How do they generalize?

        - **Instance-based**: Can simply comparing new data points to known data points

        - **Model-based learning**: Require training data to detect patterns


### Human Supervision
#### Supervised Learning
Dataset contains both inputs & corresponding labels. Tasks include:

- **Classification**: Discrete categories.

- **Regression**: Continuous values.

#### Unsupervised Learning
Identifies patterns without labeled data. Key methods are:

- **Clustering**: Group similar data points (k-means, Hierarchical Cluster Analysis (HCA), Expectation Maximization).

- **Dimensionality Reduction**: Simplify datasets without losing too much info.

    - PCA | Principal Component Analysis

    - Kernel PCA

    - LLE | Locally-Linear Embedding

    - t-SNE | t-distributed Stochastic Neighbor Embedding

- **Anomaly detection**: Identify deviations from normal behavior.

- **Association Rules** (Apriori, Eclat)

#### Semi-Supervised
Some data is labeled, some isn't. (Typically lots of unlabeled data + some labeled data)

- Most of these algorithms are combinations of unsupervised & supervised algorithms.

- Deep belief networks (DBNs), restricted Boltzmann machines (RBMs)

#### Reinforcement Learning
**Process**: The learning system (aka agent)

1. Observes environment

2. Performs actions

3. Gets rewarded
With trial-and-error, it teaches itself the best strategy (ie policy) to max reward.

### Learning Type | Batch & Online

Can they learn incrementally from a stream of incoming data?

| Learning Type | When      | **Training Process**                                                                                         | **Adaptability** |
| ------------- | --------- | ------------------------------------------------------------------------------------------------------------ | ---------------- |
| `BATCH`       | Static    | Train on the entire dataset at once                                                                          | Slow             |
| `ONLINE`      | Streaming | Model updates iteratively as new data points or mini-batches of data are received. (`Learning rate` is key.) | Fast             |
### Generalization Type | Model & Instance
How does the ML system generalize?

- **Model-Based Learning**: Use training data to build a model, then extrapolate.

- **Instance-Based Learning**: Use known problems as initial points. Predict new problems based on similarity to old ones.

    - **Examples**: Case-Based Reasoning, Radial Basis Function Networks, Locally Weighted Regression, Memory-Based Collaborative Filtering, Prototype-Based Learning
        All these methods depend heavily on **stored data** or a **local region** of the feature space. Predictions or decisions are derived **directly or indirectly from comparisons** to similar instances.

| FEATURE    | Explanation                             | **Prediction**                                                              | Adaptability                       | When                                                                               |
| ---------- | --------------------------------------- | --------------------------------------------------------------------------- | ---------------------------------- | ---------------------------------------------------------------------------------- |
| `Instance` | Learn "by heart", think look-alikes     | Using a similarity metric. Predictions are `locally-informed`. See example. | High - changes predictions quickly | Intuitive for business users<br><br>Local relationships more important than global |
| `Model`    | Learns over time with new training data | Using learned parameters                                                    | Low - adding drops to an ocean     | Generalization to unseen data is key                                               |

## Challenges of ML
> In short, since your main task is to select a learning algorithm and train it on some data, the two things that can go wrong are “bad algorithm” and “bad data.”  Pg 50

### Data Issues

1. **Quantity**: Typically need thousands of examples

2. **Quality**: Might have too much info missing, could be poorly collected

3. **Non-representative**: When old cases no longer reflect new cases. Sources:

    1. Sampling *noise*: Data is too small

    2. Sampling *bias*: Sampling method is flawed.

4. **Irrelevant features**. Solutions:

    1. *Feature selection*: Select only most useful features.

    2. *Feature extraction*: Combine existing features to produce meaningful ones.

    3. *New features*: Use external sources to create new features.

### Algorithm Issues

1. **Overfitting** solutions:

    - Select a model with fewer parameters

    - Feature reduction

    - Constrain the model (*regularization*)

    - Gather more data

    - Reduce noise in training data

2. **Underfitting**: Model is too simplistic to capture underlying patterns.

    - Select a more powerful model, with more parameters

    - Feeding better features to the learning algorithm (feature engineering)

    - Reducing the constraints on the model (e.g., reducing the regularization hyper‐parameter)
---
## Tuning & Evaluation

### Process | Testing & Validating
1) **Split**: Split data into train & test. (Usually 80% for training.)
2) **Validation set**: Use nested k-fold CV to split up training set.
3) **Train set**: Run multiple models x hyperparameters
4) **Train set**: Select models x hyperparameter combo with best performance on validation set.
5) **Test set**: Find **generalization error** for an estimate of performance on unseen data.
    1) Training good + validation bad = overfitting ([Image](https://i.imgur.com/EkW054R.png))
    2) Validation good + test bad = overfitting
    3) Validation bad + test bad = learning rate too high.

### Hyperparameter Optimization

> [!sam]
> Hyperparameters are configuration variables that tell the model what methods to use, as opposed to **model parameters** which are learned during training.

Fine-tuning hyperparameters is critical for optimal model performance:

- **Grid Search**: Exhaustive search over parameter combinations.

- **Random Search**: Randomly sample parameters to find optimal settings.

- **Bayesian Optimization**: Use probabilistic models to select parameters.

**Model-type details:**

| Type          | \# of parameters                | Complexity                       | Scalability with Data Size | Hyperparameters                                                                  |
| ------------- | ------------------------------- | -------------------------------- | -------------------------- | -------------------------------------------------------------------------------- |
| PARAMETRIC    | Fixed                           | Lower, less risk of overfitting. | Limited                    | 1. Regularization terms (L1/L2)<br>2. Learning rate<br>3. Small \# of key params |
| NONPARAMETRIC | Unconstrained, grows with size. | High, prone to overfitting.      | High                       | Focus on selecting right complexity (e.g., depth of trees).                      |

#### Code
```python
# Grid search
search = GridSearchCV(estimator = rf_classifier, param_grid = parameters, cv = 3)

# Apply to training
search.fit(x_train, y_train)  
search.best_params_

# Best combo
best = search.best_estimator_  
accuracy = evaluate(best, x_test, y_test)
```

---
### Cross-Validation
Cross-validation splits data to validate models. Methods include:

- **k-Fold**: Divides data into $k$ subsets for training and testing.

- **Nested**: Addresses hyperparameter overfitting by adding an outer validation loop.

#### Nested K-Fold
Removes overfit "leak" from evaluating on train set.

- Use when hyperparameters also need to be optimized

- Estimates generalization error of the underlying model & hyperparameters

Process

- **Inner loop**: Fits model to each training set, then select hypers over validation set

- **Outer loop**: Estimates generalization error by averaging test set scores over several dataset splits