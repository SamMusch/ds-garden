---
CoverImage: null
Covers: ML Foundations
Due: null
Function: Network
HoursDone: 50
HoursRemain: 0
Objective: Reference
Quality: ★★★★
QualityComment: Basics
ReviewFreq: 1-Month
TimeSpent: null
TimeSpent2: null
_kMDItemDisplayNameWithExtensions: ML.md
ai_abstract: null
ai_key_terms: null
aliases: null
children: 0
created: 2025-07-18
cssclasses: null
grandchildren: 0
kMDItemAlternateNames: (
kMDItemContentCreationDate: 2024-09-06 16:49:35 +0000
kMDItemContentCreationDate_Ranking: 2024-09-06 00:00:00 +0000
kMDItemContentModificationDate: 2025-05-18 14:52:44 +0000
kMDItemContentType: net.daringfireball.markdown
kMDItemContentTypeTree: (
kMDItemDateAdded: 2025-02-01 17:16:38 +0000
kMDItemDocumentIdentifier: '97063'
kMDItemFSCreatorCode: ''
kMDItemFSFinderFlags: '16'
kMDItemFSHasCustomIcon: (null)
kMDItemFSInvisible: '0'
kMDItemFSIsExtensionHidden: '1'
kMDItemFSIsStationery: (null)
kMDItemFSLabel: '0'
kMDItemFSNodeCount: (null)
kMDItemFSOwnerGroupID: '20'
kMDItemFSOwnerUserID: '502'
kMDItemFSTypeCode: ''
kMDItemInterestingDate_Ranking: 2024-12-29 00:00:00 +0000
kMDItemLastUsedDate: 2024-12-29 20:07:59 +0000
kMDItemLastUsedDate_Ranking: 2024-12-29 00:00:00 +0000
kMDItemUseCount: '12'
kMDItemUsedDates: (
kMDItemUserCreatedDate: (
kMDItemUserCreatedUserHandle: (
modified: 2025-05-18
published: true
reading_time: 7.8
source_file: ML.md
tags:

- ml_

- supervised
title: ML
word_count: 1570
---

## Deeper Links

| Notes                    | Contains                          |
| ------------------------ | --------------------------------- |
| [[ML-01-Classification]] | Models, evaluation, and some code |
| [[ML-02-Regression]]     | Evaluation                        |
| [[ML-03-Ensemble]]       | Models                            |
| [[ML-04-DL]]             |                                   |

>[!sam]
>ML components ([Source](https://learning.oreilly.com/library/view/hands-on-artificial-intelligence/9781788991063/c72aa49d-41f1-4a15-bee5-9efc9190f282.xhtml))
>- dataset
>- model
>- loss function
>- optimization technique

---

Use ML when:

- Complex problems

- Lots of data

- Fluctuating environments

### Data Mining Tasks

1. **Classification**

2. **Regression**

3. **Similarity matching**: X bought from us. Who else is likely to?

4. **Clustering**

5. **Co-occurence** (market basket analysis)

6. **Profiling**: What is the typical behavior of this segment?

7. **Link prediction**: You and x share 10 friends. She likes this person, so you prob will too.

8. **Data reduction**: Dropping unnecessary info

9. **Causal modeling**: What influences our DV?

### Data Mining Process
ML lifecycle: steps for transforming data --> actionable insights

1. **Business Understanding**: Define the problem & success criteria.

2. **Data Understanding**: How was it collected? Any implicit biases?

3. **Data Preparation**

4. **Model**

    1. **Data Preparation** (possibly)

    2. **Modeling** (possibly)

5. **Evaluate**

6. **Deploy**

---
## Types of Systems

!!! sam
    Broad categories are based on:

    - Are they trained with human supervision? (Paradigms)

        - **Supervised**: learns from labeled data

        - **Unsupervised**: finds structure in unlabeled data.

        - **Semisupervised**: uses a mix of labeled + unlabeled data.

        - **Reinforcement**: learns via rewards/penalties from interactions with an environment.

    - Can they learn incrementally on the fly?

        - **Online**: Yes

        - **Batch**: No

    - How do they generalize?

        - **Instance-based**: Take known data points --> compare new data points 

        - **Model-based learning**: Require training data to detect patterns


### Paradigms | Core 4

!!! sam
    Classify according to amount & type of supervision the system receives.

    - **Supervised Learning**

        - _Goal_: given input features, predict target values

        - _Data_: labeled dataset `(X, y)`

        - _Tasks_: classification & regression

            - _Algorithms_: linear, svm, xgboost

    - **Unsupervised Learning**

        - _Goal_: find structure or patterns in unlabeled data.

        - _Data_: unlabeled dataset

        - _Tasks_: clustering, dimensionality reduction, anomaly detection, association rules

            - _Algorithms_: DBSCAN, PCA, autoencoders

    - **Semi-Supervised Learning**

        - _Goal_: see supervised

        - _Data_: some labeled `(X, y)`, most unlabeled

        - *Tasks*: see supervised, but where labeling is expensive

            - _Algorithms_: DBNs, RBMs

    - **Reinforcement Learning**

        - _Goal_: learn a policy that selects actions to maximize long-term reward

        - _Data_: experience tuples `(state, action, reward, next_state)` gathered through interaction with an environment

        - *Tasks*: sequential decision-making, control, planning

            - _Algorithms_: Q-learning, SARSA, Policy Gradient


### Learning Type | Batch & Online

!!! sam
    Does the system learn incrementally from a stream of incoming data?

    - **Batch**: Static, train on the entire dataset at once

    - **Online**: Streaming, model updates as new data points are received. (`Learning rate` is key.)


### Generalization Type | Model & Instance

!!! sam
    How does the ML system generalize to new data?

    **Model-Based Learning**

    - *goal*: use training data to build a model, then extrapolate.

    - *data*: full dataset 

    - *learning approach*: train once → discard data → use learned model to predict

    - *prediction/generalization mechanism*: new inputs → learned model → outputs

    - *use when*: generalization matters most

    **Instance-Based Learning**

    - *goal*: memorize training instances → compare new inputs to them

    - *data*: training instances kept in memory (or efficiently indexed)

    - *learning approach*: measure similarity to stored instances → predict

    - *prediction/generalization mechanism*: find NN or do weighted vote/average

    - *use when*: local relationships matter most





## Challenges of ML

Two things can go wrong: “bad algorithm” and “bad data”.

#### Data Issues

!!! sam
    1. **Quantity**: Typically need thousands of examples

    2. **Quality**: Might have too much info missing, could be poorly collected

    3. **Non-representative**: When old cases no longer reflect new cases. Sources:

        1. Sampling *noise*: Data is too small

        2. Sampling *bias*: Sampling method is flawed.

    4. **Irrelevant features**. Solutions:

        1. *Feature selection*: Select only most useful features.

        2. *Feature extraction*: Combine existing features to produce meaningful ones.

        3. *New features*: Use external sources to create new features.


#### Algorithm Issues

!!! sam
    1. **Overfitting** solutions:

        - Select a model with fewer parameters

        - Feature reduction

        - Constrain the model (*regularization*)

        - Gather more data

        - Reduce noise in training data

    2. **Underfitting**: Model is too simplistic to capture underlying patterns.

        - Select a more powerful model, with more parameters

        - Feeding better features to the learning algorithm (feature engineering)

        - Reducing the constraints on the model (e.g., reducing the regularization hyper‐parameter)



## Tuning & Evaluation

### Process | Testing & Validating
1) **Split**: Split data into train & test. (Usually 80% for training.)
2) **Validation set**: Use nested k-fold CV to split up training set.
3) **Train set**: Run multiple models x hyperparameters
4) **Train set**: Select models x hyperparameter combo with best performance on validation set.
5) **Test set**: Find **generalization error** for an estimate of performance on unseen data.
    1) Training good + validation bad = overfitting ([Image](https://i.imgur.com/EkW054R.png))
    2) Validation good + test bad = overfitting
    3) Validation bad + test bad = learning rate too high.

### Hyperparameter Optimization

> [!sam]
> Hyperparameters are configuration variables that tell the model what methods to use, as opposed to **model parameters** which are learned during training.

Fine-tuning hyperparameters is critical for optimal model performance:

- **Grid Search**: Exhaustive search over parameter combinations.

- **Random Search**: Randomly sample parameters to find optimal settings.

- **Bayesian Optimization**: Use probabilistic models to select parameters.

**Model-type details:**

| Type          | \# of parameters                | Complexity                       | Scalability with Data Size | Hyperparameters                                                                  |
| ------------- | ------------------------------- | -------------------------------- | -------------------------- | -------------------------------------------------------------------------------- |
| PARAMETRIC    | Fixed                           | Lower, less risk of overfitting. | Limited                    | 1. Regularization terms (L1/L2)<br>2. Learning rate<br>3. Small \# of key params |
| NONPARAMETRIC | Unconstrained, grows with size. | High, prone to overfitting.      | High                       | Focus on selecting right complexity (e.g., depth of trees).                      |

#### Code
```python
# Grid search
search = GridSearchCV(estimator = rf_classifier, param_grid = parameters, cv = 3)

# Apply to training
search.fit(x_train, y_train)  
search.best_params_

# Best combo
best = search.best_estimator_  
accuracy = evaluate(best, x_test, y_test)
```

---
### Cross-Validation
Cross-validation splits data to validate models. Methods include:

- **k-Fold**: Divides data into $k$ subsets for training and testing.

- **Nested**: Addresses hyperparameter overfitting by adding an outer validation loop.

#### Nested K-Fold
Removes overfit "leak" from evaluating on train set.

- Use when hyperparameters also need to be optimized

- Estimates generalization error of the underlying model & hyperparameters

Process

- **Inner loop**: Fits model to each training set, then select hypers over validation set

- **Outer loop**: Estimates generalization error by averaging test set scores over several dataset splits