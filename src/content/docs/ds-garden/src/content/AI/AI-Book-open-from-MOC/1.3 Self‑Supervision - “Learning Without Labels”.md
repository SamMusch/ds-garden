---
title: 1.3 Self‑Supervision - “Learning Without Labels”
---


### Context
```ad-sam

- **Supervised**: learns a direct mapping from inputs to human-provided labels.
- **Unsupervised**: discovers structure or representation in unlabeled data.
- **Semi-supervised**: uses a small set of labels plus a large pool of unlabeled examples to improve performance.
- **Self-supervised (this doc)**: automatically creates its own labels from the data (e.g. masking, rotation prediction) to learn representations without manual annotation.
```


### Business Analogy
```ad-sam
**Puzzle-Building Analogy**  

1. Someone put together a 1k piece puzzle.
2. They hide a patch of 10 pieces.
3. You need to predict what those 10 pieces look like.
4. Repeat 2 & 3
5. Over time, you get better at **understanding** the overall image, all without **seeing** the full completed picture.


- Hiding puzzle pieces = Masking inputs
- Predicting them yourself = Creating your own training labels
- Getting better at the full image = Learning rich representations
```

### High-Level Concept
Self‑supervision is a way for models to generate their own “quiz” signals from raw data, eliminating the need for manually annotated examples. It powers breakthroughs in how systems understand patterns and adapt to new tasks.

### Technical Details for Data Scientists
- **Proxy Tasks**: Create artificial tasks using the data itself (e.g., mask part of an input and predict it).
- **Representation Learning**: Learn embeddings by contrasting different views of the same data point or reconstructing missing pieces.
- **Common Approaches**:  
  - Masked token prediction in text (e.g., hide 15% of words).  
  - Masked patch reconstruction in images (e.g., hide random image patches).  
  - Contrastive objectives that pull related samples together in embedding space and push others apart.

### Connections to Other Topics
- **Transfer Learning**: Use self‑supervised pretraining to initialize models before fine‑tuning on a specific task.
- **Contrastive Learning**: A popular self‑supervised method that underlies many representation‑learning models.
- **RAG and Downstream Tasks**: Better embeddings from self‑supervision improve retrieval and generation in RAG pipelines.


### Key Techniques
- **Contrastive Learning**  
  - InfoNCE loss, SimCLR, MoCo  
  - Encourages similar views to have closer embeddings than dissimilar ones
- **Masked Language Modeling (MLM)**  
  - BERT-style masking of input tokens  
  - Predicts masked tokens from context
- **Masked Image Modeling (MIM)**  
  - MAE-style masking of image patches  
  - Reconstructs missing patches

### Popular Methods
- **SimCLR** (Chen et al., 2020)  
- **MoCo** (He et al., 2020)  
- **BERT** (Devlin et al., 2019)  
- **MAE** (He et al., 2021)

