---
title: 5.1 Local vs Cloud Inference (Pros & Cons)
---
### 5.1 Local vs Cloud Inference (Pros & Cons)

**Scope**  
Decision‑guide for running model inference after training/fine‑tuning.

#### Key Dimensions

| Dimension | Local (Edge/Laptop)  | Cloud (AWS/GCP/Azure/HF) |
| --------- | -------------------- | ------------------------ |
| Latency   | Sub‑50 ms possible   | ≥ 150 ms typical         |
| Privacy   | Data stays on‑device | Data traverses network   |
| Cost      | CapEx once           | OpEx pay‑per‑request     |
| Scale     | Limited to device    | Auto‑scales globally     |
| Updates   | Manual scripts       | CI/CD pipelines          |
| Failover  | Manual (UPS)         | Multi‑zone built‑in      |

#### Heuristics

* **Choose Local** for PII, demos, offline, deterministic batch size.  
* **Choose Cloud** for bursty traffic, multi‑tenant SaaS, analytics APIs.

#### Hybrid Patterns

1. Client‑side reranking.  
2. On‑device fallback.  
3. Federated Learning.

#### Latency Test Snippet

```python
import time, requests
t0 = time.time(); _ = model(**inputs); print(f"local {1000*(time.time()-t0):.1f} ms")
print("cloud", 1000*requests.get("https://api.yourmodel.com/ping").elapsed.total_seconds(), "ms")
```
