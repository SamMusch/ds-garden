---
title: 5.2 Quick Quantization to Fit on a Laptop GPU
---
### 5.2 Quick Quantization to Fit on a  Laptop GPU

Shrink FP32/FP16 models to INT8/4‑bit so they run in ≤ 8 GB VRAM.

#### 1. Dynamic INT8 (PyTorch)

```python
from torch.ao.quantization import quantize_dynamic
model = AutoModelForCausalLM.from_pretrained("gpt2")
qmodel = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
qmodel.save_pretrained("gpt2‑int8")
```

*Memory cut*: ‑60 %, *speed*: 1.4×.

#### 2. 4‑bit GGUF (llama.cpp)

```bash
python convert.py --outfile llama7b-q4_0.gguf --wbits 4 --model llama-7b
./main -m llama7b-q4_0.gguf -p "Explain quantization"
```

#### 3. bitsandbytes + PEFT

```python
bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B", quantization_config=bnb_cfg)
```

**Gotchas**  

* Calibration data matters for vision; less for text.  
* Torch < 2.1 may need `torch.backends.quantized.engine = 'qnnpack'`.
