### 6.1 Collect & Clean Text Data Fast

A 5‑step pipeline to build quality corpora.

1. **Source**: Common Crawl, arXiv, internal SQL exports.  
2. **License/PII filters**: `scancode-toolkit`, `presidio`.  
3. **Cleaning**: HTML strip, Unicode normalise, dedup MinHash.  
4. **Language filter**: `langdetect` or fastText LID.  
5. **Save**: Parquet + Zstd.

```python
from bs4 import BeautifulSoup, Comment
import re, langdetect
def clean_html(raw):
    soup = BeautifulSoup(raw, "lxml")
    for c in soup(["script","style", Comment]): c.extract()
    txt = re.sub(r'\s+', ' ', soup.get_text(' ')).strip()
    return txt if langdetect.detect(txt) == 'en' else None
```