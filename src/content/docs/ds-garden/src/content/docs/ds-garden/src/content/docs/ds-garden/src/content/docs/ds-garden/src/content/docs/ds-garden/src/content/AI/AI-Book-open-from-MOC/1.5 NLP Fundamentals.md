---
Quality: ★
QualityComment: Why isn't this a 10?
Objective: Reference
Function: Hierarchy
ReviewFreq: 'Weekly, 1-Month, 2-Month, 3-Month'
Due: null
HoursDone: 3
HoursRemain: 11
CoverImage: null
tags:
  - ai_
TimeSpent: null
TimeSpent2: null
title: 1.5 NLP Fundamentals
---

Back/Outgoing Links
```dataview
list from [[]] and !outgoing([[]])
```

```ad-sam

Most common NLP methods

1. **Symbolic (Rule-Based):** Not really used anymore because of the advance of LLMs.

2. **ML (Statistical & Neural)**

```

# HuggingFace Course

For context: [[1.2 Transformers in Plain English]]

[HuggingFace](https://huggingface.co/learn/nlp-course/chapter1/1)  |  [Wikipedia](https://www.wikiwand.com/en/Transformer_(deep_learning_architecture))
- **NLP** | field of linguistics & ML related to language. The aim is to understand single words & the context of those words together.
- **LLMs** are DL algorithms that use **transformer models** to perform **NLP** tasks.


### 1. Transformer Models
3 major groups of transformer models: [History, 2018-2021](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg)

| Model                                                                       |      |                      |                                                            | Examples                                   | Tasks                                                                            |
| --------------------------------------------------------------------------- | ---- | -------------------- | ---------------------------------------------------------- | ------------------------------------------ | -------------------------------------------------------------------------------- |
| [Encoder](https://huggingface.co/learn/nlp-course/chapter1/5?fw=pt)         | BERT | Auto-Encoding        | Receives input, builds features.                           | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Sentence classification, named entity recognition, extractive question answering |
| [Decoder](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)         | GPT  | Auto-Regressive      | Uses encoder's features + other inputs to generate target. | CTRL, GPT, GPT-2, Transformer XL, LLaMA    | Text generation                                                                  |
| [Encoder-decoder](https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt) | BART | Sequence-to-sequence |                                                            | BART, T5, Marian, mBART                    | Summarization, translation, generative question answering                        |

```ad-sam
 All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as ***language models***. This means they have been trained on large amounts of raw text in a **self-supervised** fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!

The general pretrained model then goes through ***transfer learning*** where the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.
```


#### Transfer Learning
- *Pretraining* is the act of training a model from scratch.
- *Fine-tuning* is done **after** a model has been pretrained, using a new specific dataset.

Transformer models are built with *attention layers*, which tell the model to pay specific attention to certain words in the sentence. ([Paper](https://arxiv.org/abs/1706.03762))

**Transformer Model** Terms:
- **Architecture**: Skeleton of the model — the definition of each layer & operation within the model. (eg `BERT`)
- **Checkpoints**: The weights that will be loaded in the architecture. (eg `bert-lower-cased`)
- **Model**: Umbrella term that could mean “architecture” or “checkpoint”. This course will specify *architecture* or *checkpoint* when it matters.


### 2. Using Transformers

Moved to [[1.6 Hugging Face Workflows]]


