[Behind the pipeline](https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt) - steps - [YouTube](https://www.youtube.com/watch?v=1pedAIvTWXk&t=52s)

1. **Preprocessing with a tokenizer**:
   2. Load the tokenizer.
   3. Tokenize input text into tokens and convert to tensors.
4. **Model**:
   5. Load the pretrained model.
   6. Pass the tokenized inputs through the model to get output logits.
7. **Postprocessing the output**:
   8. Convert logits to probabilities using softmax.
   9. Map the probabilities to labels.

Breaking out steps in detail: **Tokenizer**
1. **Tokens** | Split the input into *tokens* (ie words, subwords, punctuation)
2. **Special tokens** | Add special tokens such as "sentence begin".
3. **Input IDs** | Map each token to it's unique ID from that specific pre-trained model. (This is the *checkpoint* which comes out as a *dictionary*.)

Breaking out steps in detail: **Model** ([Image](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg))
1. Input from Tokenizer
2. Transformer network
   3. **Embeddings** | convert each unique ID into vector
   4. **Layers** | manipulate vectors using attention mechanism
5. **Hidden states (features)** | 
6. **Head** | take high-dimensional vector of hidden states as input, then convert predictions to task-specific output
7. Output for post-processing