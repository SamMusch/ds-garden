---
CoverImage: null
Covers: null
Due: null
Function: null
HoursDone: null
HoursRemain: null
Objective: null
Quality: null
QualityComment: null
ReviewFreq: null
TimeSpent: null
TimeSpent2: null
_kMDItemDisplayNameWithExtensions: 2.4 Generative Models Overview.md
ai_abstract: null
ai_key_terms: []
aliases: null
children: 0
created: '2025-07-18'
cssclasses: null
grandchildren: 0
kMDItemContentCreationDate: 2025-05-20 16:24:39 +0000
kMDItemContentCreationDate_Ranking: 2025-05-20 00:00:00 +0000
kMDItemContentModificationDate: 2025-05-25 13:55:30 +0000
kMDItemContentType: net.daringfireball.markdown
kMDItemContentTypeTree: (
kMDItemDateAdded: 2025-05-20 16:37:18 +0000
kMDItemDocumentIdentifier: '167124'
kMDItemFSCreatorCode: ''
kMDItemFSFinderFlags: '0'
kMDItemFSHasCustomIcon: (null)
kMDItemFSInvisible: '0'
kMDItemFSIsExtensionHidden: '0'
kMDItemFSIsStationery: (null)
kMDItemFSLabel: '0'
kMDItemFSNodeCount: (null)
kMDItemFSOwnerGroupID: '20'
kMDItemFSOwnerUserID: '502'
kMDItemFSTypeCode: ''
kMDItemInterestingDate_Ranking: 2025-05-25 00:00:00 +0000
modified: '2025-05-25'
published: true
reading_time: 2.3
source_file: 2.4 Generative Models Overview.md
tags: null
title: 2.4 Generative Models Overview
word_count: 469
---

[Textbook | Oreilly](https://learning.oreilly.com/library/view/hands-on-artificial-intelligence/9781788991063/719394b6-6058-4ac6-89f3-c2ec26563e7a.xhtml)

## Overview

!pasted-image-20241231152903.png.md
Image comes from textbook in 2.5-autoencoders-gans-diffusionmodels.md


**Generative models** are enabling computers to have an understanding of the world. They are:
- Unsupervised
- Data generators

We'll focus on the two most popular types of models:
- The **variational autoencoder** (**VAE**)
- The **generative adversarial network** (**GAN**)
- (Also touch upon other common generative models)

**VAEs** & **GANs**. 
- Each relies on condensing data --> generating from this condensed data.
- Both are probabilistic models, meaning that they rely on inference from probability distributions in order to generate data

When writing advanced models in TensorFlow, remember:
- **Layers**: what kind you need
- **Activation function**: what type you need
- **Shape**: shape your data through the network
- **Procedures**: for example, think choosing loss functions & optimizers




## Autoencoder Overview - Separate Resource

[ChatGPT](https://chatgpt.com/share/67dee1a1-8600-8000-88c7-ffce728e06cf) - `Quick summary. Meant to be intuitive, not exhaustive.`

Autoencoders learn succinct representations. They serve as a foundation for many generative and dimensionality-reduction techniques.
Structurally, autoencoders consist of an **input layer**, a **hidden layer**, and an **output** **layer**: pasted-image-20241231143947.png.md

```ad-sam

**Intuition**

**Autoencoders**: example of encoder --> decoder.

1. I have an **idea** that I want to share. I'll explain using an example. Three examples come to mind, and I choose the simplest one.
2. The listener hears my example, and then re-generates my original **idea**.

We want the **idea** I shared to be as close as possible to the **idea** the listener understands. ("Reconstruction loss")

```

**Autoencoders** are a self-supervised approach to representation learning. They operate with two main components:
1. An **encoder** that compresses (or *encodes*) the input into a smaller, more compact representation (an “information bottleneck”).
2. A **decoder** that *reconstructs* the original input from this compact representation.
The encoder is a *recognition* network, the decoder is a *generative* network.

**Key Ideas**
- **Information Bottleneck**: The encoder forces the network to keep only *the most relevant aspects* of the data; this helps avoid simply memorizing inputs.
- **Reconstruction Loss**: Typically mean squared error or cross entropy, which measures how close the decoder’s output is to the original input.
- **Goal**: Find a balance where the model accurately reconstructs input data but doesn’t overfit or memorize it.

**Connection to PCA**
- Like **PCA**, autoencoders perform **dimensionality reduction**, taking high-dimensional data and learning a lower-dimensional representation.
- **Difference**: Autoencoders can capture **nonlinear** relationships, whereas PCA is restricted to **linear** correlations.

Manifolds
- A **manifold** is a continuous, non-intersecting surface (think of a sphere).
- In neural networks, loss functions and data structures often lie on (possibly complex) manifolds. Autoencoders learn to navigate these manifolds by mapping input data into a meaningful, compressed space and then reconstructing it back.