
Resources
- [[Deep Learning]]: Class notes, more detailed than below.
- [ChatGPT](https://chatgpt.com/share/681789dd-021c-8000-a07b-964d90d8e09d): for this markdown

---

```ad-sam
**Summary**
NNs work because they are **flexible function approximators** that learn useful **internal representations** of data by stacking layers of transformations. 
Instead of manually designing features or rules, neural nets **learn patterns directly from data** through a process of trial-and-error optimization.

| Strength                        | What It Means                                     |
| ------------------------------- | ------------------------------------------------- |
| Hierarchical representation     | Learns from simple to complex features            |
| Flexible function approximation | Can learn a wide variety of patterns and tasks    |

```

### Why NNs Work
```ad-sam
- Neural nets can **approximate almost any function**, given enough layers and data (Universal Approximation Theorem).
- They are **data-driven**: no need to hand-engineer features or rules.
- They are **modular** (think sub-task) and **scalable**: you can adjust size, depth, and architecture based on the problem.
```

### What Are NNs?
```ad-sam
At their core, NNs are inspired by the brain. They consist of layers of artificial “neurons” that **process input data** and pass signals forward. Each neuron learns to activate in response to specific patterns.

- **Input layer**: Receives raw data (e.g., pixels, words, numbers).
- **Hidden layers**: Transform and abstract the input features into higher-level representations.
- **Output layer**: Produces a prediction or decision.

Each connection has a **weight** that determines how much influence one neuron has on the next.
```

### Layered Representations
```ad-sam
The true magic of NNs lies in **stacking multiple layers**:

- **First layers** learn low-level features (e.g., edges in images).
- **Deeper layers** build on those to detect more abstract patterns (e.g., faces, objects).
- **Final layers** make task-specific predictions (e.g., cat vs. dog).

This **hierarchical learning** allows neural nets to go from raw data to meaning, automatically.
```

### Learning Through Feedback (Training)
```ad-sam

NNs improve by **learning from mistakes**. Here's how:

1. Make a guess (prediction).
2. Compare it to the correct answer (loss).
3. Adjust internal weights to reduce future error.

This process is called **gradient-based optimization**.
- (Note: Backpropagation is part of gradient-based optimization. Specifically, it’s the algorithm used to compute how to adjust the weights by calculating the gradients (step 3). Gradient-based optimization is the overall process; backpropagation is the mechanism used during that process.)
```
