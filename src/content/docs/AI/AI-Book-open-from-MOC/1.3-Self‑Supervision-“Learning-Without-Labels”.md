---
CoverImage: null
Covers: null
Due: null
Function: null
HoursDone: null
HoursRemain: null
Objective: null
Quality: null
QualityComment: null
ReviewFreq: null
TimeSpent: null
TimeSpent2: null
_kMDItemDisplayNameWithExtensions: 1.3 Self‑Supervision - “Learning Without Labels”.md
ai_abstract: null
ai_key_terms: []
aliases: null
children: 0
created: '2025-07-18'
cssclasses: null
grandchildren: 0
kMDItemContentCreationDate: 2025-05-24 15:13:55 +0000
kMDItemContentCreationDate_Ranking: 2025-05-24 00:00:00 +0000
kMDItemContentModificationDate: 2025-05-24 16:09:00 +0000
kMDItemContentType: net.daringfireball.markdown
kMDItemContentTypeTree: (
kMDItemDateAdded: 2025-05-24 15:16:58 +0000
kMDItemDocumentIdentifier: '167266'
kMDItemFSCreatorCode: ''
kMDItemFSFinderFlags: '0'
kMDItemFSHasCustomIcon: (null)
kMDItemFSInvisible: '0'
kMDItemFSIsExtensionHidden: '0'
kMDItemFSIsStationery: (null)
kMDItemFSLabel: '0'
kMDItemFSNodeCount: (null)
kMDItemFSOwnerGroupID: '20'
kMDItemFSOwnerUserID: '502'
kMDItemFSTypeCode: ''
kMDItemInterestingDate_Ranking: 2025-06-07 00:00:00 +0000
kMDItemLastUsedDate: 2025-06-07 13:58:02 +0000
kMDItemLastUsedDate_Ranking: 2025-06-07 00:00:00 +0000
kMDItemUseCount: '9'
kMDItemUsedDates: (
modified: '2025-05-24'
published: true
reading_time: 1.9
source_file: 1.3 Self‑Supervision - “Learning Without Labels”.md
tags: null
title: 1.3 Self‑Supervision   “Learning Without Labels”
word_count: 376
---

### Context
```ad-sam

- **Supervised**: learns a direct mapping from inputs to human-provided labels.
- **Unsupervised**: discovers structure or representation in unlabeled data.
- **Semi-supervised**: uses a small set of labels plus a large pool of unlabeled examples to improve performance.
- **Self-supervised (this doc)**: automatically creates its own labels from the data (e.g. masking, rotation prediction) to learn representations without manual annotation.
```


### Business Analogy
```ad-sam
**Puzzle-Building Analogy**  

1. Someone put together a 1k piece puzzle.
2. They hide a patch of 10 pieces.
3. You need to predict what those 10 pieces look like.
4. Repeat 2 & 3
5. Over time, you get better at **understanding** the overall image, all without **seeing** the full completed picture.


- Hiding puzzle pieces = Masking inputs
- Predicting them yourself = Creating your own training labels
- Getting better at the full image = Learning rich representations
```

### High-Level Concept
Self‑supervision is a way for models to generate their own “quiz” signals from raw data, eliminating the need for manually annotated examples. It powers breakthroughs in how systems understand patterns and adapt to new tasks.

### Technical Details for Data Scientists
- **Proxy Tasks**: Create artificial tasks using the data itself (e.g., mask part of an input and predict it).
- **Representation Learning**: Learn embeddings by contrasting different views of the same data point or reconstructing missing pieces.
- **Common Approaches**:  
  - Masked token prediction in text (e.g., hide 15% of words).  
  - Masked patch reconstruction in images (e.g., hide random image patches).  
  - Contrastive objectives that pull related samples together in embedding space and push others apart.

### Connections to Other Topics
- **Transfer Learning**: Use self‑supervised pretraining to initialize models before fine‑tuning on a specific task.
- **Contrastive Learning**: A popular self‑supervised method that underlies many representation‑learning models.
- **RAG and Downstream Tasks**: Better embeddings from self‑supervision improve retrieval and generation in RAG pipelines.


### Key Techniques
- **Contrastive Learning**  
  - InfoNCE loss, SimCLR, MoCo  
  - Encourages similar views to have closer embeddings than dissimilar ones
- **Masked Language Modeling (MLM)**  
  - BERT-style masking of input tokens  
  - Predicts masked tokens from context
- **Masked Image Modeling (MIM)**  
  - MAE-style masking of image patches  
  - Reconstructs missing patches

### Popular Methods
- **SimCLR** (Chen et al., 2020)  
- **MoCo** (He et al., 2020)  
- **BERT** (Devlin et al., 2019)  
- **MAE** (He et al., 2021)