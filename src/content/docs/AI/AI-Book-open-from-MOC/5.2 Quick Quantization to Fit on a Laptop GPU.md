---
title: 5.2 Quick Quantization to Fit on a Laptop GPU
created: '2025-07-18'
modified: '2025-05-20'
source_file: 5.2 Quick Quantization to Fit on a Laptop GPU.md
word_count: 124
reading_time: 0.6
children: 0
grandchildren: 0
ai_abstract: null
ai_key_terms: []
_kMDItemDisplayNameWithExtensions: 5.2 Quick Quantization to Fit on a Laptop GPU.md
kMDItemContentCreationDate: 2025-05-20 16:36:25 +0000
kMDItemContentCreationDate_Ranking: 2025-05-20 00:00:00 +0000
kMDItemContentModificationDate: 2025-05-20 16:54:07 +0000
kMDItemContentType: net.daringfireball.markdown
kMDItemContentTypeTree: (
kMDItemDateAdded: 2025-05-20 16:37:24 +0000
kMDItemDocumentIdentifier: '167129'
kMDItemFSCreatorCode: ''
kMDItemFSFinderFlags: '0'
kMDItemFSHasCustomIcon: (null)
kMDItemFSInvisible: '0'
kMDItemFSIsExtensionHidden: '0'
kMDItemFSIsStationery: (null)
kMDItemFSLabel: '0'
kMDItemFSNodeCount: (null)
kMDItemFSOwnerGroupID: '20'
kMDItemFSOwnerUserID: '502'
kMDItemFSTypeCode: ''
kMDItemInterestingDate_Ranking: 2025-05-20 00:00:00 +0000
kMDItemLastUsedDate: 2025-05-20 16:52:06 +0000
kMDItemLastUsedDate_Ranking: 2025-05-20 00:00:00 +0000
kMDItemUseCount: '9'
kMDItemUsedDates: (
Due: null
Function: null
Objective: null
Quality: null
QualityComment: null
ReviewFreq: null
CoverImage: null
HoursDone: null
HoursRemain: null
tags: null
TimeSpent: null
TimeSpent2: null
Covers: null
cssclasses: null
aliases: null
---

### 5.2 Quick Quantization to Fit on a  Laptop GPU

Shrink FP32/FP16 models to INT8/4‑bit so they run in ≤ 8 GB VRAM.

#### 1. Dynamic INT8 (PyTorch)

```python
from torch.ao.quantization import quantize_dynamic
model = AutoModelForCausalLM.from_pretrained("gpt2")
qmodel = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
qmodel.save_pretrained("gpt2‑int8")
```

*Memory cut*: ‑60 %, *speed*: 1.4×.

#### 2. 4‑bit GGUF (llama.cpp)

```bash
python convert.py --outfile llama7b-q4_0.gguf --wbits 4 --model llama-7b
./main -m llama7b-q4_0.gguf -p "Explain quantization"
```

#### 3. bitsandbytes + PEFT

```python
bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B", quantization_config=bnb_cfg)
```

**Gotchas**  

* Calibration data matters for vision; less for text.  
* Torch < 2.1 may need `torch.backends.quantized.engine = 'qnnpack'`.