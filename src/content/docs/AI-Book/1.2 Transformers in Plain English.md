---
title: 1.2 Transformers in Plain English
---

Resources
- [[NLP]] and [[HuggingFace NLP]]: Talks about transformers
- [ChatGPT](https://chatgpt.com/share/681792c6-48c0-8000-b324-765516a6ce87): This markdown

### Everyday Analogy
```ad-sam
Imagine reading a paragraph and instantly knowing which earlier sentences are important. 
That’s what transformers do — they 
1. read left-to-right AND
2. weigh relevance across the whole passage in real time
```

```ad-sam
**Transformers are the architecture behind nearly every language model today.**

Transformers marked a shift 
**FROM** sequence-based models 
**TO** attention-based models. 

They are both fast & context-aware.
```

### What is a Transformer?
```ad-sam
Transformers are a type of DL model introduced in 2017. They're best known for powering LLMs like GPT & BERT. 
Unlike older models that processed language one word at a time, transformers look at entire sentences (or documents) all at once.

**Transformers replaced recurrence with attention**, allowing models to see everything at once — a breakthrough for understanding context.
```

### Why They Matter
```ad-sam
Traditional models like RNNs had trouble (1) remembering long-term dependencies or (2) were slow to train. 
Transformers solved both problems:
- **Parallel Processing**: They process words simultaneously, not sequentially.
- **Long-Range Context**: They can "attend" to any word, no matter how far apart.
```

### Key Ingredients (Without Equations)
```ad-sam
**Self-attention is like scanning a room and deciding who’s worth listening to** — for every word in the sentence.


- **Tokens**: Break input into chunks (usually words or subwords).
- **Embeddings**: Convert those chunks into numbers.
- **Self-Attention**: Each token "looks" at others to decide what matters.
- **Layers**: These operations repeat multiple times to refine understanding.

```
